{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da4ea8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@lilianli1922/authenticating-vertex-ai-gemini-api-calls-in-python-using-service-accounts-without-gcloud-cli-e17203995ff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b6d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_credentials_from_file(service_account_path: str):\n",
    "    \"\"\"Loads Google Cloud credentials from a service account JSON file.\"\"\"\n",
    "    try:\n",
    "        return service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Service account file not found: {service_account_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading credentials from {service_account_path}: {e}\")\n",
    "\n",
    "def get_project_id_from_file(service_account_path: str) -> str:\n",
    "    \"\"\"Extracts the project_id from the service account JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(service_account_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            project_id = data.get(\"project_id\")\n",
    "            if not project_id:\n",
    "                raise ValueError(\"Key 'project_id' not found in service account file.\")\n",
    "            return project_id\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Service account file not found: {service_account_path}\")\n",
    "    except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
    "        raise RuntimeError(f\"Error reading project_id from {service_account_path}: {e}\")\n",
    "\n",
    "def generate_text_vertexai(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials, # Pass the loaded credentials object\n",
    "    prompt: str,\n",
    "    model_name: str = 'gemini-2.0-flash', # Default model, can be overridden\n",
    "    temperature: float = 0.9, # Example generation parameter\n",
    "    max_output_tokens: int = 256 # Example generation parameter\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using a Gemini model via the Vertex AI API with explicit credentials.\n",
    "\n",
    "    Args:\n",
    "        project_id: The Google Cloud project ID.\n",
    "        location: The Google Cloud region (e.g., \"us-central1\").\n",
    "        credentials: The loaded google.oauth2.service_account.Credentials object.\n",
    "        prompt: The text prompt for the model.\n",
    "        model_name: The name of the Gemini model to use (e.g., \"gemini-1.5-flash\").\n",
    "        temperature: Controls randomness (0.0-1.0). Higher values are more creative.\n",
    "        max_output_tokens: Maximum number of tokens in the generated response.\n",
    "\n",
    "    Returns:\n",
    "        The generated text from the model. Returns an empty string on errors\n",
    "        where no candidate is available. Raises exceptions for setup/auth errors.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Vertex AI for project: {project_id}, location: {location}\")\n",
    "    try:\n",
    "        vertexai.init(project=project_id, location=location, credentials=credentials)\n",
    "\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        model = GenerativeModel(model_name)\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_output_tokens\n",
    "        )\n",
    "\n",
    "        print(f\"Sending prompt: '{prompt[:50]}...'\") # Show beginning of prompt\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "        print(\"Response received.\")\n",
    "        # Basic check: Ensure candidates list is not empty and has content\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            return response.candidates[0].content.parts[0].text\n",
    "        else:\n",
    "            # Log or handle cases with no valid response (e.g., safety filters)\n",
    "            print(f\"Warning: No valid response candidates found. Response: {response}\")\n",
    "\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Vertex AI text generation: {e}\")\n",
    "        # Depending on requirements, you might re-raise, return None, or return \"\"\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27031f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "import os\n",
    "\n",
    "config = ConfigParser()\n",
    "\n",
    "KEYS_DIRECTORY = '/Volumes/T7 Shield/.keys' # Load config from external drive (so that my LLM in my IDE cannot access it (is limited anyway))\n",
    "config.read(os.path.join(KEYS_DIRECTORY, 'config.ini'))\n",
    "\n",
    "# Get config values\n",
    "PROJECT_ID = config.get('google', 'project_id')\n",
    "LOCATION = config.get('google', 'location')\n",
    "MODEL_NAME = config.get('google', 'default_model')\n",
    "\n",
    "# Make service account file path absolute\n",
    "service_account_filename = config.get('google', 'service_account_file')\n",
    "if not os.path.isabs(service_account_filename):\n",
    "    SERVICE_ACCOUNT_FILE = os.path.join(KEYS_DIRECTORY, service_account_filename)\n",
    "else:\n",
    "    SERVICE_ACCOUNT_FILE = service_account_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2199060",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = \"Provide a list of statistical twins of Albert Laszlo Barabasi?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba4d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Vertex AI for project: fengroland, location: us-central1\n",
      "Loading model: gemini-2.0-flash\n",
      "Sending prompt: 'Provide a list of statistical twins of Albert Lasz...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barolo/LLMScholar-Audits/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1758449405.509763 8310720 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received.\n",
      "\n",
      "--- Generated Text ---\n",
      "The concept of \"statistical twins\" is not a precisely defined term in statistics. It's generally used to describe individuals or entities that share similar statistical properties or patterns within a specific context. In the context of Albert-Laszlo Barabasi, a prominent network scientist, finding \"statistical twins\" would involve identifying other individuals who exhibit similar characteristics, particularly in terms of their network properties, research impact, and career trajectory.\n",
      "\n",
      "Here are a few ways to interpret \"statistical twins\" in this context, along with potential candidates based on each interpretation:\n",
      "\n",
      "**1. Research Impact & Citation Metrics:**\n",
      "\n",
      "*   **Focus:**  Individuals with a similar number of publications, citation counts, h-index, g-index, or other bibliometric measures as Barabasi. This is a quantitative approach.\n",
      "*   **Potential Candidates:**\n",
      "    *   **To identify these, you would need access to comprehensive bibliometric databases like Web of Science, Scopus, or Google Scholar.**  You would search for researchers in related fields (network science, complex systems, applied mathematics, physics, computer science) and filter by citation metrics.\n",
      "    *   It's very difficult to provide a definitive list without a specific query in these databases. Finding someone *exactly\n"
     ]
    }
   ],
   "source": [
    "# --- Authentication and Setup ---\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "        raise FileNotFoundError(f\"Critical: Service account file not found at {SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "# Load credentials object\n",
    "creds = load_credentials_from_file(SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "# Extract project ID (can also be passed directly if known)\n",
    "proj_id = get_project_id_from_file(SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "# --- Generate Text ---\n",
    "generated_text = generate_text_vertexai(\n",
    "    project_id=proj_id,\n",
    "    location=LOCATION,\n",
    "    credentials=creds,\n",
    "    prompt=USER_PROMPT,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "# --- Output ---\n",
    "if generated_text:\n",
    "    print(\"\\n--- Generated Text ---\")\n",
    "    print(generated_text)\n",
    "else:\n",
    "    print(\"\\nFailed to generate text or received an empty response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03796fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these to your existing imports\n",
    "# CORRECTED IMPORT\n",
    "from vertexai.generative_models import Tool, grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d04687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure this import is at the top of your script\n",
    "from vertexai.generative_models import Tool, grounding\n",
    "\n",
    "def generate_text_vertexai(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials,\n",
    "    prompt: str,\n",
    "    model_name: str = 'gemini-1.0-pro', # A common model that supports this\n",
    "    temperature: float = 0.9,\n",
    "    max_output_tokens: int = 256,\n",
    "    use_google_search: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using a Gemini model, with an option\n",
    "    to ground the model with Google Search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        vertexai.init(project=project_id, location=location, credentials=credentials)\n",
    "        model = GenerativeModel(model_name)\n",
    "        \n",
    "        tools = None\n",
    "        if use_google_search:\n",
    "            print(\"Grounding with Google Search is ENABLED.\")\n",
    "            # This tool definition is correct for modern library versions\n",
    "            google_search_tool = Tool.from_google_search_retrieval(grounding.GoogleSearchRetrieval())\n",
    "            tools = [google_search_tool]\n",
    "\n",
    "        print(f\"Sending prompt: '{prompt[:50]}...'\")\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        print(\"Response received.\")\n",
    "        if response.grounding_metadata:\n",
    "             print(\"Grounding Metadata Found:\", response.grounding_metadata)\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    except Exception as e:\n",
    "        # The error message you received is an example of what this will catch\n",
    "        print(f\"An error occurred during Vertex AI text generation: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a809f8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running the same prompt with Google Search grounding enabled...\n",
      "Grounding with Google Search is ENABLED.\n",
      "Sending prompt: 'Provide a list of statistical twins of Albert Lasz...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758449408.193105 8310720 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during Vertex AI text generation: 400 Unable to submit request because google_search_retrieval is not supported; please use google_search field instead. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini\n",
      "\n",
      "Failed to generate text with grounding or received an empty response.\n"
     ]
    }
   ],
   "source": [
    "# --- [NEW CELL] Generate Text WITH Grounding ---\n",
    "\n",
    "print(\"\\nRunning the same prompt with Google Search grounding enabled...\")\n",
    "\n",
    "# Call the same function, but set use_google_search to True\n",
    "grounded_text = generate_text_vertexai(\n",
    "    project_id=proj_id,\n",
    "    location=LOCATION,\n",
    "    credentials=creds,\n",
    "    prompt=USER_PROMPT,\n",
    "    model_name=MODEL_NAME, # Using the same model\n",
    "    use_google_search=True # This is the only change needed for the call\n",
    ")\n",
    "\n",
    "# --- Output ---\n",
    "if grounded_text:\n",
    "    print(\"\\n--- Generated Text (with Google Search) ---\")\n",
    "    print(grounded_text)\n",
    "else:\n",
    "    print(\"\\nFailed to generate text with grounding or received an empty response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8934d045",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m config = GenerateContentConfig(tools=tools)\n\u001b[32m      7\u001b[39m client = genai.Client()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.5-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/genai/models.py:6565\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   6563\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   6564\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m6565\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6566\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   6567\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6568\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   6569\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/genai/models.py:5377\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5374\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   5375\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m5377\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5378\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   5379\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   5382\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5383\u001b[39m ):\n\u001b[32m   5384\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/genai/_api_client.py:1290\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1282\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1286\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1287\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1288\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1289\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1290\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1291\u001b[39m   response_body = (\n\u001b[32m   1292\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1293\u001b[39m   )\n\u001b[32m   1294\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/genai/_api_client.py:1126\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1123\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.2-macos-aarch64-none/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/genai/_api_client.py:1069\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1067\u001b[39m \u001b[38;5;66;03m# If using proj/location, fetch ADC\u001b[39;00m\n\u001b[32m   1068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vertexai \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.project \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.location):\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m   http_request.headers[\u001b[33m'\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_access_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1070\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._credentials \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._credentials.quota_project_id:\n\u001b[32m   1071\u001b[39m     http_request.headers[\u001b[33m'\u001b[39m\u001b[33mx-goog-user-project\u001b[39m\u001b[33m'\u001b[39m] = (\n\u001b[32m   1072\u001b[39m         \u001b[38;5;28mself\u001b[39m._credentials.quota_project_id\n\u001b[32m   1073\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/genai/_api_client.py:905\u001b[39m, in \u001b[36mBaseApiClient._access_token\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sync_auth_lock:\n\u001b[32m    904\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._credentials:\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m     \u001b[38;5;28mself\u001b[39m._credentials, project = \u001b[43mload_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.project:\n\u001b[32m    907\u001b[39m       \u001b[38;5;28mself\u001b[39m.project = project\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/genai/_api_client.py:183\u001b[39m, in \u001b[36mload_auth\u001b[39m\u001b[34m(project)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_auth\u001b[39m(*, project: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]) -> Tuple[Credentials, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    182\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Loads google auth credentials and project id.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m   credentials, loaded_project_id = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[no-untyped-call]\u001b[39;49;00m\n\u001b[32m    184\u001b[39m \u001b[43m      \u001b[49m\u001b[43mscopes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://www.googleapis.com/auth/cloud-platform\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m project:\n\u001b[32m    188\u001b[39m     project = loaded_project_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/auth/_default.py:685\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    677\u001b[39m             _LOGGER.warning(\n\u001b[32m    678\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNo project ID could be determined. Consider running \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    679\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    680\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33menvironment variable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    681\u001b[39m                 environment_vars.PROJECT,\n\u001b[32m    682\u001b[39m             )\n\u001b[32m    683\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Tool, GoogleSearch, GenerateContentConfig\n",
    "\n",
    "tools = [Tool(google_search=GoogleSearch())]\n",
    "config = GenerateContentConfig(tools=tools)\n",
    "\n",
    "client = genai.Client()\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config=config\n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "833cc061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI SDK version: 1.115.0\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "print(f\"Vertex AI SDK version: {vertexai.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb504369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.2 environment at: /Users/barolo/LLMScholar-Audits/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m43 packages\u001b[0m \u001b[2min 722ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2mAudited \u001b[1m43 packages\u001b[0m \u001b[2min 0.10ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c06f811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.2 environment at: /Users/barolo/LLMScholar-Audits/.venv\u001b[0m\n",
      "Name: google-cloud-aiplatform\n",
      "Version: 1.115.0\n",
      "Location: /Users/barolo/LLMScholar-Audits/.venv/lib/python3.13/site-packages\n",
      "Requires: docstring-parser, google-api-core, google-auth, google-cloud-bigquery, google-cloud-resource-manager, google-cloud-storage, google-genai, packaging, proto-plus, protobuf, pydantic, shapely, typing-extensions\n",
      "Required-by: vertexai\n"
     ]
    }
   ],
   "source": [
    "!uv pip show google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738b88ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING WITH REST API APPROACH\n",
      "============================================================\n",
      "Error in REST API call: ('invalid_scope: Invalid OAuth scope or ID token audience provided.', {'error': 'invalid_scope', 'error_description': 'Invalid OAuth scope or ID token audience provided.'})\n",
      "REST API call failed\n",
      "\n",
      "============================================================\n",
      "CHECKING PERMISSIONS\n",
      "============================================================\n",
      "Permission check failed: ('invalid_scope: Invalid OAuth scope or ID token audience provided.', {'error': 'invalid_scope', 'error_description': 'Invalid OAuth scope or ID token audience provided.'})\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "def generate_with_grounding_rest_api(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials,\n",
    "    prompt: str,\n",
    "    model_name: str = 'gemini-2.0-flash'\n",
    "):\n",
    "    \"\"\"\n",
    "    Use the REST API directly for Google Search grounding\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Refresh credentials if needed\n",
    "        if not credentials.valid:\n",
    "            credentials.refresh(Request())\n",
    "        \n",
    "        # Construct the REST API URL\n",
    "        url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model_name}:generateContent\"\n",
    "        \n",
    "        # Request headers\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {credentials.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Request body\n",
    "        request_body = {\n",
    "            \"contents\": [{\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [{\"text\": prompt}]\n",
    "            }],\n",
    "            \"tools\": [{\"googleSearch\": {}}],\n",
    "            \"generationConfig\": {\n",
    "                \"temperature\": 1.0,\n",
    "                \"maxOutputTokens\": 2048\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Making REST API call to: {url}\")\n",
    "        print(f\"Request body: {json.dumps(request_body, indent=2)}\")\n",
    "        \n",
    "        # Make the request\n",
    "        response = requests.post(url, headers=headers, json=request_body)\n",
    "        \n",
    "        print(f\"Response status code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"Error response: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in REST API call: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with REST API\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING WITH REST API APPROACH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rest_result = generate_with_grounding_rest_api(\n",
    "    project_id=proj_id,\n",
    "    location=LOCATION,\n",
    "    credentials=creds,\n",
    "    prompt=USER_PROMPT,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "if rest_result:\n",
    "    print(\"\\n--- REST API Response ---\")\n",
    "    print(json.dumps(rest_result, indent=2))\n",
    "    \n",
    "    # Extract the main response\n",
    "    if 'candidates' in rest_result and rest_result['candidates']:\n",
    "        candidate = rest_result['candidates'][0]\n",
    "        if 'content' in candidate and 'parts' in candidate['content']:\n",
    "            generated_text = candidate['content']['parts'][0]['text']\n",
    "            print(f\"\\n--- Generated Text ---\")\n",
    "            print(generated_text)\n",
    "            \n",
    "        # Extract grounding metadata\n",
    "        if 'groundingMetadata' in candidate:\n",
    "            grounding = candidate['groundingMetadata']\n",
    "            print(f\"\\n--- Grounding Information ---\")\n",
    "            \n",
    "            if 'webSearchQueries' in grounding:\n",
    "                print(f\"Search queries: {grounding['webSearchQueries']}\")\n",
    "                \n",
    "            if 'groundingChunks' in grounding:\n",
    "                print(f\"Number of sources: {len(grounding['groundingChunks'])}\")\n",
    "                for i, chunk in enumerate(grounding['groundingChunks']):\n",
    "                    if 'web' in chunk:\n",
    "                        print(f\"  {i+1}. {chunk['web'].get('title', 'No title')}\")\n",
    "                        print(f\"     URL: {chunk['web'].get('uri', 'No URL')}\")\n",
    "else:\n",
    "    print(\"REST API call failed\")\n",
    "\n",
    "# Let's also check your current permissions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING PERMISSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def check_permissions(credentials, project_id):\n",
    "    \"\"\"Check what permissions your service account has\"\"\"\n",
    "    try:\n",
    "        if not credentials.valid:\n",
    "            credentials.refresh(Request())\n",
    "            \n",
    "        # Test basic Vertex AI access\n",
    "        url = f\"https://aiplatform.googleapis.com/v1/projects/{project_id}/locations\"\n",
    "        headers = {\"Authorization\": f\"Bearer {credentials.token}\"}\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        print(f\"Basic Vertex AI access: {response.status_code == 200}\")\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Permission check failed: {e}\")\n",
    "\n",
    "check_permissions(creds, proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ad474bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading credentials with proper scopes...\n",
      "\n",
      "============================================================\n",
      "TESTING SDK APPROACH WITH PROPER SCOPES\n",
      "============================================================\n",
      "Trying tool format 1: [{'googleSearch': {}}]\n",
      "Format 1 failed: Unexpected tool type: {'googleSearch': {}}.\n",
      "Trying tool format 2: [{'google_search': {}}]\n",
      "Format 2 failed: Unexpected tool type: {'google_search': {}}.\n",
      "Trying tool format 3: [{'googleSearchRetrieval': {}}]\n",
      "Format 3 failed: Unexpected tool type: {'googleSearchRetrieval': {}}.\n",
      "All SDK formats failed\n",
      "SDK approach failed, trying REST API...\n",
      "\n",
      "============================================================\n",
      "TESTING REST API APPROACH WITH PROPER SCOPES\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barolo/LLMScholar-Audits/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REST API failed: ('No access token in response.', {'id_token': 'eyJhbGciOiJSUzI1NiIsImtpZCI6IjA3ZjA3OGYyNjQ3ZThjZDAxOWM0MGRhOTU2OWU0ZjUyNDc5OTEwOTQiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJodHRwczovL3d3dy5nb29nbGVhcGlzLmNvbS9hdXRoL2Nsb3VkLXBsYXRmb3JtLGh0dHBzOi8vd3d3Lmdvb2dsZWFwaXMuY29tL2F1dGgvYWlwbGF0Zm9ybSIsImF6cCI6ImxsbXNjaG9sYXJAZmVuZ3JvbGFuZC5pYW0uZ3NlcnZpY2VhY2NvdW50LmNvbSIsImVtYWlsIjoibGxtc2Nob2xhckBmZW5ncm9sYW5kLmlhbS5nc2VydmljZWFjY291bnQuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTc1ODQ1MzQ1MCwiaWF0IjoxNzU4NDQ5ODUwLCJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJzdWIiOiIxMDM2MzMxODE4MDIzNDQ1NDAzNTYifQ.GkoPOj5GLijXcpcWkOTkp0KaYV83a64g_8Zw_VrEwhwPtRvkV2HhR8SUUb2Lro7QrJGE4mD1oOkWM24u1wAV44xCYWR2ibGY63UCKt8EjXbwo3m_igy6Zuduzs9WpyACrXsBB7oUAJYGmjOKURf519yB91VNwKTC_D7bgWqJ0ebw3hdN0Tqr6XbXLISW64uXoHbaGjuNDE4vz0TWZgGiyd2DBaKt0MzV_UN65UpEalZ1SHJ2eaBCsgLzKD7uWAmyd0InDZsgLZfpWTyhCRaRr7VFUY0KNxEKX0S2AZKRCScyUap8ITtE9NJKtk46jHm9nvHigbBEyXlddU6uxjUytg'})\n",
      "Both approaches failed\n",
      "\n",
      "============================================================\n",
      "CHECKING GROUNDING AVAILABILITY\n",
      "============================================================\n",
      "Availability check failed: ('No access token in response.', {'id_token': 'eyJhbGciOiJSUzI1NiIsImtpZCI6IjA3ZjA3OGYyNjQ3ZThjZDAxOWM0MGRhOTU2OWU0ZjUyNDc5OTEwOTQiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJodHRwczovL3d3dy5nb29nbGVhcGlzLmNvbS9hdXRoL2Nsb3VkLXBsYXRmb3JtLGh0dHBzOi8vd3d3Lmdvb2dsZWFwaXMuY29tL2F1dGgvYWlwbGF0Zm9ybSIsImF6cCI6ImxsbXNjaG9sYXJAZmVuZ3JvbGFuZC5pYW0uZ3NlcnZpY2VhY2NvdW50LmNvbSIsImVtYWlsIjoibGxtc2Nob2xhckBmZW5ncm9sYW5kLmlhbS5nc2VydmljZWFjY291bnQuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsImV4cCI6MTc1ODQ1MzQ1MCwiaWF0IjoxNzU4NDQ5ODUwLCJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJzdWIiOiIxMDM2MzMxODE4MDIzNDQ1NDAzNTYifQ.GkoPOj5GLijXcpcWkOTkp0KaYV83a64g_8Zw_VrEwhwPtRvkV2HhR8SUUb2Lro7QrJGE4mD1oOkWM24u1wAV44xCYWR2ibGY63UCKt8EjXbwo3m_igy6Zuduzs9WpyACrXsBB7oUAJYGmjOKURf519yB91VNwKTC_D7bgWqJ0ebw3hdN0Tqr6XbXLISW64uXoHbaGjuNDE4vz0TWZgGiyd2DBaKt0MzV_UN65UpEalZ1SHJ2eaBCsgLzKD7uWAmyd0InDZsgLZfpWTyhCRaRr7VFUY0KNxEKX0S2AZKRCScyUap8ITtE9NJKtk46jHm9nvHigbBEyXlddU6uxjUytg'})\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# STEP 1: Fix your credentials loading to include proper scopes\n",
    "def load_credentials_with_scopes(service_account_path: str):\n",
    "    \"\"\"Loads Google Cloud credentials with proper scopes for grounding.\"\"\"\n",
    "    try:\n",
    "        # Define the required scopes for Vertex AI and grounding\n",
    "        scopes = [\n",
    "            'https://www.googleapis.com/auth/cloud-platform',\n",
    "            'https://www.googleapis.com/auth/aiplatform'\n",
    "        ]\n",
    "        \n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            service_account_path, \n",
    "            scopes=scopes\n",
    "        )\n",
    "        return credentials\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Service account file not found: {service_account_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading credentials from {service_account_path}: {e}\")\n",
    "\n",
    "# Load credentials with proper scopes\n",
    "print(\"Loading credentials with proper scopes...\")\n",
    "creds_with_scopes = load_credentials_with_scopes(SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "# STEP 2: Try the corrected SDK approach first\n",
    "def generate_with_grounding_sdk_v2(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials,\n",
    "    prompt: str,\n",
    "    model_name: str = 'gemini-2.0-flash'\n",
    "):\n",
    "    \"\"\"Try the SDK approach with corrected tool format\"\"\"\n",
    "    try:\n",
    "        vertexai.init(project=project_id, location=location, credentials=credentials)\n",
    "        \n",
    "        model = GenerativeModel(model_name)\n",
    "        \n",
    "        # Try different tool formats\n",
    "        tool_formats = [\n",
    "            [{\"googleSearch\": {}}],  # Format from docs\n",
    "            [{\"google_search\": {}}], # Alternative format\n",
    "            [{\"googleSearchRetrieval\": {}}], # Another possible format\n",
    "        ]\n",
    "        \n",
    "        for i, tools in enumerate(tool_formats):\n",
    "            try:\n",
    "                print(f\"Trying tool format {i+1}: {tools}\")\n",
    "                \n",
    "                response = model.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config=GenerationConfig(temperature=1.0, max_output_tokens=2048),\n",
    "                    tools=tools\n",
    "                )\n",
    "                \n",
    "                print(f\"SUCCESS with format {i+1}!\")\n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Format {i+1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"All SDK formats failed\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SDK approach failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# STEP 3: REST API with proper scopes\n",
    "def generate_with_grounding_rest_v2(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials,\n",
    "    prompt: str,\n",
    "    model_name: str = 'gemini-2.0-flash'\n",
    "):\n",
    "    \"\"\"REST API approach with proper credentials\"\"\"\n",
    "    try:\n",
    "        # Refresh credentials if needed\n",
    "        if not credentials.valid:\n",
    "            credentials.refresh(Request())\n",
    "        \n",
    "        url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models/{model_name}:generateContent\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {credentials.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        request_body = {\n",
    "            \"contents\": [{\n",
    "                \"role\": \"user\", \n",
    "                \"parts\": [{\"text\": prompt}]\n",
    "            }],\n",
    "            \"tools\": [{\"googleSearch\": {}}],\n",
    "            \"generationConfig\": {\n",
    "                \"temperature\": 1.0,\n",
    "                \"maxOutputTokens\": 2048\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Making REST API call...\")\n",
    "        response = requests.post(url, headers=headers, json=request_body)\n",
    "        \n",
    "        print(f\"Status code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"REST API Error: {response.text}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"REST API failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# STEP 4: Test both approaches\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING SDK APPROACH WITH PROPER SCOPES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sdk_result = generate_with_grounding_sdk_v2(\n",
    "    project_id=proj_id,\n",
    "    location=LOCATION,\n",
    "    credentials=creds_with_scopes,\n",
    "    prompt=USER_PROMPT,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "if sdk_result:\n",
    "    print(\"SDK SUCCESS!\")\n",
    "    if sdk_result.candidates and sdk_result.candidates[0].content.parts:\n",
    "        print(\"\\n--- SDK Generated Text ---\")\n",
    "        print(sdk_result.candidates[0].content.parts[0].text)\n",
    "        \n",
    "        if hasattr(sdk_result.candidates[0], 'grounding_metadata'):\n",
    "            print(\"\\n--- SDK Grounding Metadata ---\")\n",
    "            print(sdk_result.candidates[0].grounding_metadata)\n",
    "else:\n",
    "    print(\"SDK approach failed, trying REST API...\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TESTING REST API APPROACH WITH PROPER SCOPES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    rest_result = generate_with_grounding_rest_v2(\n",
    "        project_id=proj_id,\n",
    "        location=LOCATION,\n",
    "        credentials=creds_with_scopes,\n",
    "        prompt=USER_PROMPT,\n",
    "        model_name=MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    if rest_result:\n",
    "        print(\"REST API SUCCESS!\")\n",
    "        print(json.dumps(rest_result, indent=2))\n",
    "    else:\n",
    "        print(\"Both approaches failed\")\n",
    "\n",
    "# STEP 5: Check if grounding is available in your region/project\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING GROUNDING AVAILABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def check_grounding_availability(credentials, project_id, location):\n",
    "    \"\"\"Check if grounding is available\"\"\"\n",
    "    try:\n",
    "        if not credentials.valid:\n",
    "            credentials.refresh(Request())\n",
    "            \n",
    "        # Check if the API endpoint responds\n",
    "        url = f\"https://{location}-aiplatform.googleapis.com/v1/projects/{project_id}/locations/{location}/publishers/google/models\"\n",
    "        headers = {\"Authorization\": f\"Bearer {credentials.token}\"}\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        print(f\"API endpoint accessible: {response.status_code == 200}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Your project has access to Vertex AI\")\n",
    "            print(\"If grounding still fails, it might be:\")\n",
    "            print(\"1. Feature not enabled in your project\")\n",
    "            print(\"2. Regional availability issue\")\n",
    "            print(\"3. Billing account limitations\")\n",
    "        else:\n",
    "            print(f\"API access issue: {response.status_code} - {response.text}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Availability check failed: {e}\")\n",
    "\n",
    "check_grounding_availability(creds_with_scopes, proj_id, LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "440bc913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Google-Search Grounded Text ---\n",
      "{'candidates': [{'content': {'role': 'model', 'parts': [{'text': 'The term \"statistical twins\" in the context of Albert-Lszl Barabsi refers to individuals with similar patterns of scientific achievement and recognition. Identifying such \"twins\" involves analyzing factors beyond just talent, such as the timing and placement of their work within cultural and academic centers, and their ability to stand out from their peers.\\n\\nWhile a definitive list of Barabsi\\'s statistical twins is not readily available, the following individuals have close associations with him through co-authorship or doctoral advising:\\n\\n*   **Ginestra Bianconi**\\n*   **Reka Albert**\\n*   **Cesar Hidalgo**\\n*   **Dashun Wang**\\n\\n'}]}, 'finishReason': 'STOP', 'groundingMetadata': {'webSearchQueries': ['Albert Laszlo Barabasi statistical twins'], 'searchEntryPoint': {'renderedContent': '<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHfvZdDsLq9urCD_lddZGHoCVVTYAspdzj0swymaBk9ZteHPxnGbvpb3UvivBcmrbrNRAlLBEModc_wRJBIOuDcAOVSvNSMagHd-n1xAVIvSk3qCOmXVILVlo-xeOqTUfaHOtPSzHyCDa9X8rd-tQyQp2ztyAWJB6618OyFO1clWja4jFzxEYuuXJeGl-3kmL4_X8DD6nXF_b7YtkRntPjNubJ4i8HYA2kctpY=\">Albert Laszlo Barabasi statistical twins</a>\\n  </div>\\n</div>\\n'}, 'groundingChunks': [{'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGcZvv0fj-78blQmx4kquV1xkywN7j08FiBOgqAsuYOrEXHx1UiBk_txPHTUOGFeyZ43vO08xZ8bJHcp02wV0ZwkmwGH7WHJbo_n7X8z7sLofVspKXzhxlETQS1D9JLDwsIe_nw3ck=', 'title': 'youtube.com', 'domain': 'youtube.com'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEr4qKhq7Jm8LeAI4gj4Lofhj0bJr1GwYnP01cSguqMflD8jnbTAeLV_QSostHGEF4YR3JCLH-dFRKL_EPSRvOTLFzHW--gXi-WagvBXbqCjtykkNu_EfguFndSjGrOPEJMUGqQmgXa6V3caMp1PCzqmvuz4I7egFBdxXUMS-o=', 'title': 'wikipedia.org', 'domain': 'wikipedia.org'}}], 'groundingSupports': [{'segment': {'endIndex': 158, 'text': 'The term \"statistical twins\" in the context of Albert-Lszl Barabsi refers to individuals with similar patterns of scientific achievement and recognition'}, 'groundingChunkIndices': [0], 'confidenceScores': [0.015624122]}, {'segment': {'startIndex': 160, 'endIndex': 364, 'text': 'Identifying such \"twins\" involves analyzing factors beyond just talent, such as the timing and placement of their work within cultural and academic centers, and their ability to stand out from their peers'}, 'groundingChunkIndices': [0], 'confidenceScores': [0.07478961]}, {'segment': {'startIndex': 554, 'endIndex': 579, 'text': '*   **Ginestra Bianconi**'}, 'groundingChunkIndices': [1], 'confidenceScores': [0.5886653]}, {'segment': {'startIndex': 580, 'endIndex': 599, 'text': '*   **Reka Albert**'}, 'groundingChunkIndices': [1], 'confidenceScores': [0.6611413]}, {'segment': {'startIndex': 600, 'endIndex': 621, 'text': '*   **Cesar Hidalgo**'}, 'groundingChunkIndices': [1], 'confidenceScores': [0.5713139]}, {'segment': {'startIndex': 622, 'endIndex': 641, 'text': '*   **Dashun Wang**'}, 'groundingChunkIndices': [1], 'confidenceScores': [0.8736719]}], 'retrievalMetadata': {}}}], 'usageMetadata': {'promptTokenCount': 14, 'candidatesTokenCount': 135, 'totalTokenCount': 149, 'trafficType': 'ON_DEMAND', 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 14}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 135}]}, 'modelVersion': 'gemini-2.0-flash', 'createTime': '2025-09-21T10:27:53.211922Z', 'responseId': 'KdPPaNL3DMH_hMIPlK_P0Ak'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from google.oauth2 import service_account\n",
    "from google.auth.transport.requests import AuthorizedSession\n",
    "\n",
    "def generate_text_with_google_search(\n",
    "    service_account_path,\n",
    "    project_id,\n",
    "    location,\n",
    "    model_id,\n",
    "    prompt\n",
    "):\n",
    "    # Load credentials\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    scoped_credentials = credentials.with_scopes(['https://www.googleapis.com/auth/cloud-platform'])\n",
    "\n",
    "    api_url = (\n",
    "        f\"https://{location}-aiplatform.googleapis.com/v1/projects/\"\n",
    "        f\"{project_id}/locations/{location}/publishers/google/models/{model_id}:generateContent\"\n",
    "    )\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Create the payload\n",
    "    payload = {\n",
    "        \"contents\": [\n",
    "            {\"role\": \"user\", \"parts\": [{\"text\": prompt}]}\n",
    "        ],\n",
    "        \"tools\": [\n",
    "            {\"googleSearch\": {}}\n",
    "        ],\n",
    "        \"model\": api_url.split('/publishers')[0]  # Just for completeness; actual param may differ\n",
    "    }\n",
    "    \n",
    "    session = AuthorizedSession(scoped_credentials)\n",
    "    response = session.post(api_url, headers=headers, data=json.dumps(payload))\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    return result\n",
    "\n",
    "# Example usage in notebook:\n",
    "model_id = MODEL_NAME  # e.g. \"gemini-2.5-flash\"\n",
    "prompt = USER_PROMPT\n",
    "grounded_text = generate_text_with_google_search(\n",
    "    service_account_path=SERVICE_ACCOUNT_FILE,\n",
    "    project_id=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    model_id=model_id,\n",
    "    prompt=prompt\n",
    ")\n",
    "print(\"\\n--- Google-Search Grounded Text ---\")\n",
    "print(grounded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1983b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758450709.712022 8310720 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "ename": "PermissionDenied",
     "evalue": "403 Permission denied on resource project google. [reason: \"CONSUMER_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"aiplatform.googleapis.com\"\n}\nmetadata {\n  key: \"containerInfo\"\n  value: \"google\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/google\"\n}\n, locale: \"en-US\"\nmessage: \"Permission denied on resource project google.\"\n, links {\n  description: \"Google developers console\"\n  url: \"https://console.developers.google.com\"\n}\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_InactiveRpcError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:332\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    329\u001b[39m call = \u001b[38;5;28mself\u001b[39m._interceptor.intercept_unary_unary(\n\u001b[32m    330\u001b[39m     continuation, client_call_details, request\n\u001b[32m    331\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/grpc/_channel.py:440\u001b[39m, in \u001b[36m_InactiveRpcError.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/grpc/_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1192\u001b[39m state, call = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1193\u001b[39m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1194\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/grpc/_channel.py:1009\u001b[39m, in \u001b[36m_end_unary_response_blocking\u001b[39m\u001b[34m(state, call, with_call, deadline)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[31m_InactiveRpcError\u001b[39m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Permission denied on resource project google.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.251.36.170:443 {grpc_message:\"Permission denied on resource project google.\", grpc_status:7}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mPermissionDenied\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Use the full resource name for GPT OSS 20B\u001b[39;00m\n\u001b[32m     31\u001b[39m gpt_oss_model_full_name = \u001b[33m\"\u001b[39m\u001b[33mprojects/google/locations/us-central1/publishers/openai/models/gpt-oss-20b\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m gpt_oss_text = \u001b[43mgenerate_text_gpt_oss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproj_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOCATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUSER_PROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgpt_oss_model_full_name\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- GPT-OSS-20B Generated Text ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(gpt_oss_text)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mgenerate_text_gpt_oss\u001b[39m\u001b[34m(project_id, location, credentials, prompt, model_name, temperature, max_output_tokens)\u001b[39m\n\u001b[32m     16\u001b[39m model = GenerativeModel(model_name)\n\u001b[32m     17\u001b[39m config = GenerationConfig(\n\u001b[32m     18\u001b[39m     temperature=temperature,\n\u001b[32m     19\u001b[39m     max_output_tokens=max_output_tokens\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.candidates \u001b[38;5;129;01mand\u001b[39;00m response.candidates[\u001b[32m0\u001b[39m].content.parts:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.candidates[\u001b[32m0\u001b[39m].content.parts[\u001b[32m0\u001b[39m].text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:710\u001b[39m, in \u001b[36m_GenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, tools, tool_config, labels, stream)\u001b[39m\n\u001b[32m    701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_content_streaming(\n\u001b[32m    702\u001b[39m         contents=contents,\n\u001b[32m    703\u001b[39m         generation_config=generation_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    707\u001b[39m         labels=labels,\n\u001b[32m    708\u001b[39m     )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:833\u001b[39m, in \u001b[36m_GenerativeModel._generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, tools, tool_config, labels)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generates content.\u001b[39;00m\n\u001b[32m    807\u001b[39m \n\u001b[32m    808\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m \u001b[33;03m    A single GenerationResponse object\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    825\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    826\u001b[39m     contents=contents,\n\u001b[32m    827\u001b[39m     generation_config=generation_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    831\u001b[39m     labels=labels,\n\u001b[32m    832\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m gapic_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prediction_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_response(gapic_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:2302\u001b[39m, in \u001b[36mPredictionServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m   2299\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m   2301\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2307\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2309\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m   2310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLMScholar-Audits/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(*args, **kwargs)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mPermissionDenied\u001b[39m: 403 Permission denied on resource project google. [reason: \"CONSUMER_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"aiplatform.googleapis.com\"\n}\nmetadata {\n  key: \"containerInfo\"\n  value: \"google\"\n}\nmetadata {\n  key: \"consumer\"\n  value: \"projects/google\"\n}\n, locale: \"en-US\"\nmessage: \"Permission denied on resource project google.\"\n, links {\n  description: \"Google developers console\"\n  url: \"https://console.developers.google.com\"\n}\n]"
     ]
    }
   ],
   "source": [
    "model_name = \"projects/google/locations/us-central1/publishers/openai/models/gpt-oss-20b\"\n",
    "\n",
    "def generate_text_gpt_oss(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials,\n",
    "    prompt: str,\n",
    "    model_name: str = None,\n",
    "    temperature: float = 0.9,\n",
    "    max_output_tokens: int = 256\n",
    ") -> str:\n",
    "    import vertexai\n",
    "    from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "\n",
    "    vertexai.init(project=project_id, location=location, credentials=credentials)\n",
    "    model = GenerativeModel(model_name)\n",
    "    config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_output_tokens\n",
    "    )\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=config\n",
    "    )\n",
    "    if response.candidates and response.candidates[0].content.parts:\n",
    "        return response.candidates[0].content.parts[0].text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Use the full resource name for GPT OSS 20B\n",
    "model_name = \"gpt-oss-20b\"\n",
    "\n",
    "gpt_oss_model_full_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/{model_name}\"\n",
    "\n",
    "gpt_oss_text = generate_text_gpt_oss(\n",
    "    project_id=proj_id,\n",
    "    location=LOCATION,\n",
    "    credentials=creds,\n",
    "    prompt=USER_PROMPT,\n",
    "    model_name=gpt_oss_model_full_name\n",
    ")\n",
    "print(\"\\n--- GPT-OSS-20B Generated Text ---\")\n",
    "print(gpt_oss_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0baa5445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach with the newer Gen AI SDK\n",
    "# First install: pip install --upgrade google-genai\n",
    "\n",
    "# Then set environment variables:\n",
    "import os\n",
    "os.environ['GOOGLE_CLOUD_PROJECT'] = proj_id\n",
    "os.environ['GOOGLE_CLOUD_LOCATION'] = LOCATION  \n",
    "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True'\n",
    "\n",
    "# Then try with the new SDK..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d62283b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing gemini-2.0-flash...\n",
      "Initializing Vertex AI for project: fengroland, location: us-central1\n",
      "Loading model: gemini-2.0-flash\n",
      "Sending prompt: 'Who is Albert Laszlo Barabasi?...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758447476.704043 8278473 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received.\n",
      " gemini-2.0-flash: Albert-Lszl Barabsi is a Romanian-born Hungarian-American physicist, network scientist, and autho...\n",
      "\n",
      " Testing gemini-2.5-flash...\n",
      "Initializing Vertex AI for project: fengroland, location: us-central1\n",
      "Loading model: gemini-2.5-flash\n",
      "Sending prompt: 'Who is Albert Laszlo Barabasi?...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758447481.480502 8278473 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received.\n",
      " gemini-2.5-flash: **Albert-Lszl Barabsi** is a highly influential Hungarian-American physicist, network...\n",
      "\n",
      " Testing gemini-2.5-pro...\n",
      "Initializing Vertex AI for project: fengroland, location: us-central1\n",
      "Loading model: gemini-2.5-pro\n",
      "Sending prompt: 'Who is Albert Laszlo Barabasi?...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758447485.253193 8278473 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received.\n",
      " gemini-2.5-pro: Of course. Albert-Lszl Barabsi is a renowned Hungarian...\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Quick test - just change the model name (for Gemini models)\n",
    "USER_PROMPT = \"Who is Albert Laszlo Barabasi?\"\n",
    "\n",
    "# Test different Gemini models\n",
    "models_to_test = [\n",
    "    \"gemini-2.0-flash\",\n",
    "    \"gemini-2.5-flash\", \n",
    "    \"gemini-2.5-pro\"\n",
    "]\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"\\n Testing {model}...\")\n",
    "    \n",
    "    generated_text = generate_text_vertexai(\n",
    "        project_id=PROJECT_ID,\n",
    "        location=LOCATION,\n",
    "        credentials=creds,\n",
    "        prompt=USER_PROMPT,\n",
    "        model_name=model,\n",
    "        max_output_tokens=512  # Keep it short for testing\n",
    "    )\n",
    "    \n",
    "    if generated_text:\n",
    "        print(f\" {model}: {generated_text[:100]}...\")\n",
    "    else:\n",
    "        print(f\" {model}: Failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd32e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing anthropic/claude-3-5-haiku@20241022...\n",
      "Error with anthropic/claude-3-5-haiku@20241022: ('invalid_scope: Invalid OAuth scope or ID token audience provided.', {'error': 'invalid_scope', 'error_description': 'Invalid OAuth scope or ID token audience provided.'})\n",
      " anthropic/claude-3-5-haiku@20241022: Failed\n",
      "\n",
      " Testing anthropic/claude-3-7-sonnet@20250219...\n",
      "Error with anthropic/claude-3-7-sonnet@20250219: ('invalid_scope: Invalid OAuth scope or ID token audience provided.', {'error': 'invalid_scope', 'error_description': 'Invalid OAuth scope or ID token audience provided.'})\n",
      " anthropic/claude-3-7-sonnet@20250219: Failed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Option 2: Test Claude models (different API approach)\n",
    "import openai\n",
    "from google.auth import transport\n",
    "\n",
    "def test_claude_model(model_name, prompt):\n",
    "    \"\"\"Test Claude models via OpenAI-compatible API\"\"\"\n",
    "    try:\n",
    "        # Get GCP token\n",
    "        auth_request = transport.requests.Request()\n",
    "        creds.refresh(auth_request)\n",
    "        \n",
    "        client = openai.OpenAI(\n",
    "            api_key=creds.token,\n",
    "            base_url=f\"https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{LOCATION}/endpoints/openapi\"\n",
    "        )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=512\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {model_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test Claude models\n",
    "claude_models = [\n",
    "    \"anthropic/claude-3-5-haiku@20241022\",\n",
    "    \"anthropic/claude-3-7-sonnet@20250219\"\n",
    "]\n",
    "\n",
    "for model in claude_models:\n",
    "    print(f\"\\n Testing {model}...\")\n",
    "    result = test_claude_model(model, USER_PROMPT)\n",
    "    if result:\n",
    "        print(f\" {model}: {result[:100]}...\")\n",
    "    else:\n",
    "        print(f\" {model}: Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70285e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " About Open Source Models:\n",
      "----------------------------------------\n",
      "Open source models work differently:\n",
      " They need to be DEPLOYED first (not just enabled)\n",
      " You deploy them to endpoints (costs compute)\n",
      " Then you call the endpoint for inference\n",
      " MaaS models are pre-deployed and managed by Google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758447835.384731 8278473 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Found 0 deployed endpoints:\n"
     ]
    }
   ],
   "source": [
    "def check_open_source_models():\n",
    "    \"\"\"Check if open source models are available\"\"\"\n",
    "    \n",
    "    print(\"\\n About Open Source Models:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Open source models work differently:\")\n",
    "    print(\" They need to be DEPLOYED first (not just enabled)\")\n",
    "    print(\" You deploy them to endpoints (costs compute)\")\n",
    "    print(\" Then you call the endpoint for inference\")\n",
    "    print(\" MaaS models are pre-deployed and managed by Google\")\n",
    "    \n",
    "    # Test if we can list endpoints\n",
    "    try:\n",
    "        from google.cloud import aiplatform\n",
    "        \n",
    "        aiplatform.init(project=PROJECT_ID, location=LOCATION, credentials=creds)\n",
    "        endpoints = aiplatform.Endpoint.list()\n",
    "        \n",
    "        print(f\"\\n Found {len(endpoints)} deployed endpoints:\")\n",
    "        for endpoint in endpoints:\n",
    "            print(f\"  - {endpoint.display_name} ({endpoint.name})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not list endpoints: {e}\")\n",
    "\n",
    "check_open_source_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc5d9fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_garden' from 'vertexai.preview' (/Users/barolo/LLMScholar-Audits/.venv/lib/python3.13/site-packages/vertexai/preview/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvertexai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreview\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_garden\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Example: Deploy Llama \u001b[39;00m\n\u001b[32m      4\u001b[39m model = model_garden.OpenModel(\u001b[33m\"\u001b[39m\u001b[33mmeta/llama-3-1-8b-instruct\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'model_garden' from 'vertexai.preview' (/Users/barolo/LLMScholar-Audits/.venv/lib/python3.13/site-packages/vertexai/preview/__init__.py)"
     ]
    }
   ],
   "source": [
    "from vertexai.preview import model_garden\n",
    "\n",
    "# Example: Deploy Llama \n",
    "model = model_garden.OpenModel(\"meta/llama-3-1-8b-instruct\")\n",
    "endpoint = model.deploy()  # This costs money while running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c7ca5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'genai' from 'google' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenerateContentConfig, GoogleSearch, Tool\n\u001b[32m      4\u001b[39m response = client.models.generate_content(\n\u001b[32m      5\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     contents=\u001b[33m\"\u001b[39m\u001b[33mWhen is the next total solar eclipse in the US?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     )\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'genai' from 'google' (unknown location)"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from vertexai.generative_models import Tool\n",
    "\n",
    "# Add this before model.generate_content():\n",
    "google_search_tool = Tool.from_google_search_retrieval()\n",
    "tools = [google_search_tool]\n",
    "\n",
    "# Change your generate_content call to:\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config=generation_config,\n",
    "    tools=tools  # Add this parameter\n",
    ")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Tell me a list of statistical twins of Albert Laszlo Barabasi?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[Tool(google_search=GoogleSearch())]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "253781d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this import at the top with your other imports\n",
    "from vertexai.generative_models import Tool\n",
    "\n",
    "# Modify your existing generate_text_vertexai function\n",
    "def generate_text_vertexai(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials,\n",
    "    prompt: str,\n",
    "    model_name: str = 'gemini-2.0-flash',\n",
    "    temperature: float = 0.9,\n",
    "    max_output_tokens: int = 256,\n",
    "    use_google_search: bool = False  # ADD THIS NEW PARAMETER\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using a Gemini model via the Vertex AI API with explicit credentials.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Vertex AI for project: {project_id}, location: {location}\")\n",
    "    try:\n",
    "        vertexai.init(project=project_id, location=location, credentials=credentials)\n",
    "\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        model = GenerativeModel(model_name)\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_output_tokens\n",
    "        )\n",
    "\n",
    "        # ADD THESE LINES FOR GOOGLE SEARCH\n",
    "        tools = None\n",
    "        if use_google_search:\n",
    "            from vertexai.generative_models import grounding\n",
    "            google_search_tool = Tool.from_google_search_retrieval(\n",
    "                grounding.GoogleSearchRetrieval()\n",
    "            )\n",
    "            tools = [google_search_tool]\n",
    "            print(\" Google Search enabled\")\n",
    "\n",
    "        print(f\"Sending prompt: '{prompt[:50]}...'\")\n",
    "        \n",
    "        # MODIFY THIS LINE TO INCLUDE TOOLS\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config,\n",
    "            tools=tools  # ADD THIS\n",
    "        )\n",
    "\n",
    "        print(\"Response received.\")\n",
    "        # Basic check: Ensure candidates list is not empty and has content\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            return response.candidates[0].content.parts[0].text\n",
    "        else:\n",
    "            # Log or handle cases with no valid response (e.g., safety filters)\n",
    "            print(f\"Warning: No valid response candidates found. Response: {response}\")\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Vertex AI text generation: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbdd8680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Vertex AI for project: fengroland, location: us-central1\n",
      "Loading model: gemini-2.0-flash\n",
      "An error occurred during Vertex AI text generation: type object 'grounding' has no attribute 'Retrieval'\n",
      "\n",
      "Failed to generate text or received an empty response.\n"
     ]
    }
   ],
   "source": [
    "# Add this import at the top with your other imports\n",
    "from vertexai.generative_models import Tool, grounding\n",
    "\n",
    "# Modify your existing generate_text_vertexai function\n",
    "def generate_text_vertexai(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials,\n",
    "    prompt: str,\n",
    "    model_name: str = 'gemini-2.0-flash',\n",
    "    temperature: float = 0.9,\n",
    "    max_output_tokens: int = 256,\n",
    "    use_google_search: bool = False  # ADD THIS NEW PARAMETER\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using a Gemini model via the Vertex AI API with explicit credentials.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Vertex AI for project: {project_id}, location: {location}\")\n",
    "    try:\n",
    "        vertexai.init(project=project_id, location=location, credentials=credentials)\n",
    "\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        model = GenerativeModel(model_name)\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_output_tokens\n",
    "        )\n",
    "\n",
    "        # ADD THESE LINES FOR GOOGLE SEARCH  \n",
    "        tools = None\n",
    "        if use_google_search:\n",
    "            # For Gemini 2.0+ models, use the new google_search syntax\n",
    "            google_search_tool = Tool.from_retrieval(\n",
    "                grounding.Retrieval(google_search=grounding.GoogleSearch())\n",
    "            )\n",
    "            tools = [google_search_tool]\n",
    "            print(\" Google Search enabled\")\n",
    "\n",
    "        print(f\"Sending prompt: '{prompt[:50]}...'\")\n",
    "        \n",
    "        # MODIFY THIS LINE TO INCLUDE TOOLS\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config,\n",
    "            tools=tools  # ADD THIS\n",
    "        )\n",
    "\n",
    "        print(\"Response received.\")\n",
    "        # Basic check: Ensure candidates list is not empty and has content\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            return response.candidates[0].content.parts[0].text\n",
    "        else:\n",
    "            # Log or handle cases with no valid response (e.g., safety filters)\n",
    "            print(f\"Warning: No valid response candidates found. Response: {response}\")\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Vertex AI text generation: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Your existing working code stays the same, just add use_google_search=True\n",
    "USER_PROMPT = \"What are the latest developments in quantum computing in 2025?\"\n",
    "\n",
    "# --- Authentication and Setup ---\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "        raise FileNotFoundError(f\"Critical: Service account file not found at {SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "# Load credentials object\n",
    "creds = load_credentials_from_file(SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "# Extract project ID (can also be passed directly if known)\n",
    "proj_id = get_project_id_from_file(SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "# --- Generate Text WITH Google Search ---\n",
    "generated_text = generate_text_vertexai(\n",
    "    project_id=proj_id,\n",
    "    location=LOCATION,\n",
    "    credentials=creds,\n",
    "    prompt=USER_PROMPT,\n",
    "    model_name=MODEL_NAME,\n",
    "    use_google_search=True  # ADD THIS LINE\n",
    ")\n",
    "\n",
    "# --- Output ---\n",
    "if generated_text:\n",
    "    print(\"\\n--- Generated Text ---\")\n",
    "    print(generated_text)\n",
    "else:\n",
    "    print(\"\\nFailed to generate text or received an empty response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dac0d8a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'generate_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Just test this directly in a cell:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat happened in AI news today?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     generation_config=GenerationConfig(temperature=\u001b[32m0.7\u001b[39m, max_output_tokens=\u001b[32m200\u001b[39m),\n\u001b[32m      5\u001b[39m     tools=[Tool(google_search={})]\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'generate_content'"
     ]
    }
   ],
   "source": [
    "# Just test this directly in a cell:\n",
    "response = model.generate_content(\n",
    "    \"What happened in AI news today?\",\n",
    "    generation_config=GenerationConfig(temperature=0.7, max_output_tokens=200),\n",
    "    tools=[Tool(google_search={})]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMScholar-Audits (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
