{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da4ea8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@lilianli1922/authenticating-vertex-ai-gemini-api-calls-in-python-using-service-accounts-without-gcloud-cli-e17203995ff1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b6d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
    "from google.oauth2 import service_account\n",
    "import json\n",
    "import os\n",
    "\n",
    "def load_credentials_from_file(service_account_path: str):\n",
    "    \"\"\"Loads Google Cloud credentials from a service account JSON file.\"\"\"\n",
    "    try:\n",
    "        return service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Service account file not found: {service_account_path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading credentials from {service_account_path}: {e}\")\n",
    "\n",
    "def get_project_id_from_file(service_account_path: str) -> str:\n",
    "    \"\"\"Extracts the project_id from the service account JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(service_account_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            project_id = data.get(\"project_id\")\n",
    "            if not project_id:\n",
    "                raise ValueError(\"Key 'project_id' not found in service account file.\")\n",
    "            return project_id\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Service account file not found: {service_account_path}\")\n",
    "    except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
    "        raise RuntimeError(f\"Error reading project_id from {service_account_path}: {e}\")\n",
    "\n",
    "def generate_text_vertexai(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    credentials, # Pass the loaded credentials object\n",
    "    prompt: str,\n",
    "    model_name: str = 'gemini-2.0-flash', # Default model, can be overridden\n",
    "    temperature: float = 0.9, # Example generation parameter\n",
    "    max_output_tokens: int = 256 # Example generation parameter\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using a Gemini model via the Vertex AI API with explicit credentials.\n",
    "\n",
    "    Args:\n",
    "        project_id: The Google Cloud project ID.\n",
    "        location: The Google Cloud region (e.g., \"us-central1\").\n",
    "        credentials: The loaded google.oauth2.service_account.Credentials object.\n",
    "        prompt: The text prompt for the model.\n",
    "        model_name: The name of the Gemini model to use (e.g., \"gemini-1.5-flash\").\n",
    "        temperature: Controls randomness (0.0-1.0). Higher values are more creative.\n",
    "        max_output_tokens: Maximum number of tokens in the generated response.\n",
    "\n",
    "    Returns:\n",
    "        The generated text from the model. Returns an empty string on errors\n",
    "        where no candidate is available. Raises exceptions for setup/auth errors.\n",
    "    \"\"\"\n",
    "    print(f\"Initializing Vertex AI for project: {project_id}, location: {location}\")\n",
    "    try:\n",
    "        vertexai.init(project=project_id, location=location, credentials=credentials)\n",
    "\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        model = GenerativeModel(model_name)\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_output_tokens\n",
    "        )\n",
    "\n",
    "        print(f\"Sending prompt: '{prompt[:50]}...'\") # Show beginning of prompt\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "\n",
    "        print(\"Response received.\")\n",
    "        # Basic check: Ensure candidates list is not empty and has content\n",
    "        if response.candidates and response.candidates[0].content.parts:\n",
    "            return response.candidates[0].content.parts[0].text\n",
    "        else:\n",
    "            # Log or handle cases with no valid response (e.g., safety filters)\n",
    "            print(f\"Warning: No valid response candidates found. Response: {response}\")\n",
    "\n",
    "            return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Vertex AI text generation: {e}\")\n",
    "        # Depending on requirements, you might re-raise, return None, or return \"\"\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "config = ConfigParser()\n",
    "\n",
    "config.read('.keys/config.ini')\n",
    "PROJECT_ID = config.get('google', 'project_id')\n",
    "SERVICE_ACCOUNT_FILE = config.get('google', 'service_account_file')\n",
    "LOCATION = config.get('google', 'location')\n",
    "MODEL_NAME = config.get('google', 'default_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2199060",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROMPT = \"Who is Albert Laszlo Barabasi?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba4d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Vertex AI for project: fengroland, location: us-central1\n",
      "Loading model: gemini-2.0-flash\n",
      "Sending prompt: 'Who is Albert Laszlo Barabasi?...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/espinl/anaconda3/envs/py311LLMEval/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response received.\n",
      "\n",
      "--- Generated Text ---\n",
      "Albert-László Barabási is a prominent physicist and network scientist, best known for his pioneering work in the field of network science, specifically for his discovery of scale-free networks. Here's a more detailed breakdown:\n",
      "\n",
      "*   **Professional Background:** Barabási is a Hungarian-born Romanian-American physicist. He holds the position of Robert Gray Dodge Professor of Network Science and Distinguished University Professor at Northeastern University, where he directs the Center for Complex Network Research. He also holds appointments at the Central European University in Budapest and is a member of the Hungarian Academy of Sciences.\n",
      "\n",
      "*   **Key Contributions:** His most significant contribution is the discovery of **scale-free networks**.  These networks are characterized by a power-law degree distribution, meaning that a small number of nodes (hubs) have a very high number of connections, while most nodes have only a few connections.  This is in contrast to random networks, where nodes tend to have a similar number of connections. This discovery, detailed in his seminal 1999 paper \"Emergence of Scaling in Random Networks,\" co-authored with Réka Albert, revolutionized our understanding of complex systems.\n",
      "\n",
      "*   **\"Preferential Attachment\":**  He and Albert also proposed the \"prefer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Authentication and Setup ---\n",
    "if not os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "        raise FileNotFoundError(f\"Critical: Service account file not found at {SERVICE_ACCOUNT_FILE}\")\n",
    "\n",
    "# Load credentials object\n",
    "creds = load_credentials_from_file(SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "# Extract project ID (can also be passed directly if known)\n",
    "proj_id = get_project_id_from_file(SERVICE_ACCOUNT_FILE)\n",
    "\n",
    "# --- Generate Text ---\n",
    "generated_text = generate_text_vertexai(\n",
    "    project_id=proj_id,\n",
    "    location=LOCATION,\n",
    "    credentials=creds,\n",
    "    prompt=USER_PROMPT,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "# --- Output ---\n",
    "if generated_text:\n",
    "    print(\"\\n--- Generated Text ---\")\n",
    "    print(generated_text)\n",
    "else:\n",
    "    print(\"\\nFailed to generate text or received an empty response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2406011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311LLMEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
