#!/usr/bin/env python3
"""
Test script to demonstrate time-based batching with early retry handling.

This shows how the new system:
1. Sends batches every 60s regardless of API response time
2. Handles early failures and adds them to next batch immediately  
3. Optimizes throughput when APIs are slow
"""

import asyncio
import time
import random
from utils.smart_queue import SmartQueue

async def mock_slow_executor(experiment_pair):
    """
    Mock executor that simulates variable API response times and failures.
    
    This helps demonstrate the optimization:
    - Some calls return quickly (30s) 
    - Some calls are slow (90s)
    - Some calls fail and need retry
    """
    category, variable = experiment_pair
    task_id = f"{category}:{variable}"
    
    # Simulate variable response times
    if "fast" in variable:
        response_time = random.uniform(10, 30)  # Fast responses
    elif "slow" in variable:
        response_time = random.uniform(70, 120)  # Slow responses  
    else:
        response_time = random.uniform(30, 90)  # Mixed speeds
    
    print(f"üïê {task_id} - Starting (will take {response_time:.1f}s)")
    await asyncio.sleep(response_time)
    
    # Simulate failure rate
    if random.random() < 0.2:  # 20% failure rate
        print(f"‚ùå {task_id} - Failed after {response_time:.1f}s")
        raise Exception(f"Mock failure for {task_id}")
    
    print(f"‚úÖ {task_id} - Completed after {response_time:.1f}s")
    return f"Success: {task_id}"

async def test_time_based_batching():
    """Demonstrate the time-based batching optimization"""
    
    print("üß™ TESTING TIME-BASED BATCHING")
    print("=" * 60)
    print("This test simulates:")
    print("üì§ Batches sent every 60s (regardless of completion time)")
    print("‚ö° Fast failures added to next batch immediately")
    print("üêå Slow APIs don't block subsequent batches")
    print("=" * 60)
    
    # Create smart queue with small rate limit for testing
    queue = SmartQueue(rate_limit=5, logger=None)
    
    # Add tasks with different expected speeds
    test_experiments = [
        # Fast tasks (should complete in ~30s)
        ("model_a", [("speed", "fast_1"), ("speed", "fast_2"), ("speed", "fast_3")]),
        
        # Mixed speed tasks  
        ("model_b", [("speed", "mixed_1"), ("speed", "mixed_2"), ("speed", "mixed_3")]),
        
        # Slow tasks (will take 70-120s)
        ("model_c", [("speed", "slow_1"), ("speed", "slow_2"), ("speed", "slow_3")]),
        
        # More mixed tasks to fill multiple batches
        ("model_d", [("speed", "mixed_4"), ("speed", "mixed_5")]),
    ]
    
    # Add all experiments to queue
    for model_name, experiments in test_experiments:
        queue.add_model_tasks(model_name, experiments, mock_slow_executor)
        print(f"‚ûï Added {len(experiments)} tasks for {model_name}")
    
    print(f"\nüìä Total tasks: {queue.get_queue_status()['total_pending']}")
    print("üöÄ Starting time-based processing...\n")
    
    # Process with time-based batching
    start_time = time.time()
    results = await queue.process_all()
    total_time = time.time() - start_time
    
    print("\n" + "=" * 60)
    print("üìä RESULTS ANALYSIS")
    print("=" * 60)
    
    stats = results['stats']
    
    print(f"‚è±Ô∏è  Total Time: {total_time:.1f}s ({total_time/60:.1f} minutes)")
    print(f"üì¶ Batches Processed: {stats['batches_processed']}")
    print(f"‚úÖ Successful Tasks: {stats['completed_tasks']}")
    print(f"‚ùå Failed Tasks: {stats['failed_tasks']}")
    print(f"üîÑ Retry Tasks: {stats['retry_tasks']}")
    print(f"üì° Total API Calls: {stats['total_api_calls']}")
    print(f"‚ö° Efficiency: {stats['efficiency_percent']:.1f}%")
    
    # Calculate theoretical vs actual time
    total_tasks = stats['completed_tasks'] + stats['failed_tasks']
    old_system_time = (total_tasks / 5) * 60  # Old system: wait for each batch
    time_saved = max(0, old_system_time - total_time)
    
    print(f"\nüöÄ OPTIMIZATION IMPACT:")
    print(f"   Old system (wait for completion): ~{old_system_time/60:.1f} minutes")
    print(f"   New system (time-based): {total_time/60:.1f} minutes") 
    print(f"   Time saved: {time_saved/60:.1f} minutes ({time_saved/old_system_time*100:.1f}% improvement)")
    
    # Show batch timing details
    print(f"\nüìà BATCHING DETAILS:")
    print(f"   Average calls per batch: {stats['total_api_calls']/stats['batches_processed']:.1f}")
    print(f"   Wasted slots: {stats['wasted_slots']}")
    print(f"   Rate limit utilization: {(stats['total_api_calls']/(stats['batches_processed']*5))*100:.1f}%")

async def run_comparison():
    """Run a quick comparison between old and new systems"""
    print("\n" + "=" * 60)
    print("üî¨ COMPARISON: Old vs New Batching")
    print("=" * 60)
    
    print("üìà Expected improvements:")
    print("   ‚Ä¢ Slow API calls (>60s): ~40-60% faster")
    print("   ‚Ä¢ Fast failures: Immediate retry in next batch")  
    print("   ‚Ä¢ Mixed workloads: Better resource utilization")
    print("   ‚Ä¢ Multi-model runs: Optimal cross-model batching")
    
    print("\nüéØ Key scenarios where this shines:")
    print("   ‚Ä¢ APIs with variable response times")
    print("   ‚Ä¢ Workloads with retry-able failures")  
    print("   ‚Ä¢ Multiple models with different queue sizes")
    print("   ‚Ä¢ Long-running experiment batches")

if __name__ == "__main__":
    print("üß™ Time-Based Batching Test Suite")
    print("=" * 60)
    
    asyncio.run(test_time_based_batching())
    asyncio.run(run_comparison())