{
  "metadata": {
    "version": "2.0",
    "created": "2025-10-07",
    "last_updated": "2025-12-16",
    "description": "LLM model configurations for LLMScholar audit"
  },
  "models": {
    "llama-3.3-8b": {
      "model": "meta-llama/llama-3.1-8b-instruct",
      "max_attempts": 3,
      "temperature": 0.0,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "deepinfra/bf16",
      "quantization": "fp16",
      "sub_provider_model_endpoint": "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "latency": "0.31s",
      "throughput": "26.51tps",
      "total_context": "131.1K",
      "max_output": "16.4K",
      "host_country": "US",
      "input_price": "0.03",
      "output_price": "0.05"
    },
    "llama-4-scout": {
      "model": "meta-llama/llama-4-scout",
      "max_attempts": 3,
      "temperature": 1.0,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "deepinfra/fp8",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "latency": "0.60s",
      "throughput": "29.52tps",
      "total_context": "327.7K",
      "max_output": "16.4K",
      "host_country": "US",
      "input_price": "0.08",
      "output_price": "0.30"
    },
    "llama-4-mav": {
      "model": "meta-llama/llama-4-maverick",
      "max_attempts": 3,
      "temperature": 0.5,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "deepinfra/base",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
      "latency": "1.47s",
      "throughput": "55.20tps",
      "total_context": "1.05M",
      "max_output": "16.4K",
      "host_country": "US",
      "input_price": "0.15",
      "output_price": "0.60"
    },
    "gpt-oss-20b": {
      "model": "openai/gpt-oss-20b",
      "max_attempts": 3,
      "temperature": 0.0,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "ncompass",
      "quantization": "",
      "sub_provider_model_endpoint": "openai/gpt-oss-20b",
      "latency": "1.36s",
      "throughput": "186.7tps",
      "total_context": "131.1K",
      "max_output": "131.1K",
      "host_country": "US",
      "input_price": "0.04",
      "output_price": "0.15"
    },
    "gpt-oss-120b": {
      "model": "openai/gpt-oss-120b",
      "max_attempts": 3,
      "temperature": 0.0,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "ncompass",
      "quantization": "",
      "sub_provider_model_endpoint": "openai/gpt-oss-120b",
      "latency": "4.11s",
      "throughput": "6.63tps",
      "total_context": "131K",
      "max_output": "131K",
      "host_country": "US",
      "input_price": "0.05",
      "output_price": "0.28"
    },
    "qwen3-8b": {
      "model": "qwen/qwen3-8b",
      "max_attempts": 3,
      "temperature": 0.5,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "novita/fp8",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "qwen/qwen3-8b-fp8",
      "latency": "1.25s",
      "throughput": "50.67tps",
      "total_context": "128K",
      "max_output": "20K",
      "host_country": "US",
      "input_price": "0.035",
      "output_price": "0.138"
    },
    "qwen3-14b": {
      "model": "qwen/qwen3-14b",
      "max_attempts": 3,
      "temperature": 0.0,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "deepinfra/fp8",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "Qwen/Qwen3-14B",
      "latency": "0.91s",
      "throughput": "49.12tps",
      "total_context": "41.0K",
      "max_output": "41.0K",
      "host_country": "US",
      "input_price": "0.06",
      "output_price": "0.24"
    },
    "qwen3-32b": {
      "model": "qwen/qwen3-32b",
      "max_attempts": 3,
      "temperature": 0.25,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "deepinfra/fp8",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "Qwen/Qwen3-32B",
      "latency": "0.78s",
      "throughput": "105.3tps",
      "total_context": "41.0K",
      "max_output": "41.0K",
      "host_country": "US",
      "input_price": "0.10",
      "output_price": "0.28"
    },
    "qwen3-30b-a3b-2507": {
      "model": "qwen/qwen3-30b-a3b-instruct-2507",
      "max_attempts": 3,
      "temperature": 0.75,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "atlas-cloud/bf16",
      "quantization": "bf16",
      "sub_provider_model_endpoint": "Qwen/Qwen3-30B-A3B-Instruct-2507",
      "latency": "0.64s",
      "throughput": "81.66tps",
      "total_context": "131.1K",
      "max_output": "131.1K",
      "host_country": "US",
      "input_price": "0.09",
      "output_price": "0.45"
    },
    "qwen3-235b-a22b-2507": {
      "model": "qwen/qwen3-235b-a22b-2507",
      "max_attempts": 3,
      "temperature": 1.5,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "wandb/bf16",
      "quantization": "bf16",
      "sub_provider_model_endpoint": "Qwen/Qwen3-235B-A22B-Instruct-2507",
      "latency": "4.90s",
      "throughput": "18.45tps",
      "total_context": "262.1K",
      "max_output": "262.1K",
      "host_country": "US",
      "input_price": "0.10",
      "output_price": "0.10"
    },
    "gemma-3-12b-it": {
      "model": "google/gemma-3-12b-it",
      "max_attempts": 3,
      "temperature": 0.25,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "novita/bf16",
      "quantization": "bf16",
      "sub_provider_model_endpoint": "google/gemma-3-12b-it",
      "latency": "1.09s",
      "throughput": "72.03tps",
      "total_context": "131.1K",
      "max_output": "8.2K",
      "host_country": "US",
      "input_price": "0.05",
      "output_price": "0.10"
    },
    "gemma-3-27b-it": {
      "model": "google/gemma-3-27b-it",
      "max_attempts": 3,
      "temperature": 0.25,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "deepinfra/fp8",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "google/gemma-3-27b-it",
      "latency": "0.74s",
      "throughput": "31.73tps",
      "total_context": "131.1K",
      "max_output": "16.4K",
      "host_country": "US",
      "input_price": "0.09",
      "output_price": "0.16"
    },
    "mistral-small-3.2-24b": {
      "model": "mistralai/mistral-small-3.2-24b-instruct",
      "max_attempts": 3,
      "temperature": 0.75,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "mistral",
      "quantization": "",
      "sub_provider_model_endpoint": "mistral-small-2506",
      "latency": "0.35s",
      "throughput": "82.16tps",
      "total_context": "131.1K",
      "max_output": "131.1K",
      "host_country": "FR",
      "input_price": "0.10",
      "output_price": "0.30"
    },
    "mistral-medium-3": {
      "model": "mistralai/mistral-medium-3",
      "max_attempts": 3,
      "temperature": 1.5,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "mistral",
      "quantization": "",
      "sub_provider_model_endpoint": "mistral-medium-2505",
      "latency": "0.38s",
      "throughput": "51.23tps",
      "total_context": "131.1K",
      "max_output": "131.1K",
      "host_country": "FR",
      "input_price": "0.40",
      "output_price": "2"
    },
    "llama-3.1-70b": {
      "model": "meta-llama/llama-3.1-70b-instruct",
      "max_attempts": 3,
      "temperature": 0.5,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "deepinfra/base",
      "quantization": "bf16",
      "sub_provider_model_endpoint": "meta-llama/Meta-Llama-3.1-70B-Instruct",
      "latency": "0.47s",
      "throughput": "19.63tps",
      "total_context": "131.1K",
      "max_output": "131.1K",
      "host_country": "US",
      "input_price": "0.40",
      "output_price": "0.40"
    },
    "llama-3.3-70b": {
      "model": "meta-llama/llama-3.3-70b-instruct",
      "max_attempts": 3,
      "temperature": 0.0,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "novita/bf16",
      "quantization": "bf16",
      "sub_provider_model_endpoint": "meta-llama/llama-3.3-70b-instruct",
      "latency": "0.87s",
      "throughput": "33.42tps",
      "total_context": "131.1K",
      "max_output": "120K",
      "host_country": "US",
      "input_price": "0.13",
      "output_price": "0.39"
    },
    "llama-3.1-405b": {
      "model": "meta-llama/llama-3.1-405b-instruct",
      "max_attempts": 3,
      "temperature": 1.0,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "together/fp8",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
      "latency": "0.33s",
      "throughput": "27.40tps",
      "total_context": "130.8K",
      "max_output": "130.8K",
      "host_country": "US",
      "input_price": "3.50",
      "output_price": "3.50"
    },
    "grok-4-fast": {
      "model": "x-ai/grok-4-fast",
      "max_attempts": 3,
      "temperature": 0.25,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "xai",
      "quantization": "",
      "sub_provider_model_endpoint": "grok-4-fast-non-reasoning",
      "latency": "5.02s",
      "throughput": "89.19tps",
      "total_context": "2M",
      "max_output": "30K",
      "host_country": "US",
      "input_price": "0.40",
      "output_price": "1"
    },
    "deepseek-chat-v3.1": {
      "model": "deepseek/deepseek-chat-v3.1",
      "max_attempts": 3,
      "temperature": 0.0,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "siliconflow/fp8",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "deepseek-ai/DeepSeek-V3.1",
      "latency": "1.93s",
      "throughput": "21.37tps",
      "total_context": "163.8K",
      "max_output": "163.8K",
      "host_country": "",
      "input_price": "0.27",
      "output_price": "1"
    },
    "deepseek-r1-0528": {
      "model": "deepseek/deepseek-r1-0528",
      "max_attempts": 3,
      "temperature": 0.25,
      "max_temperature": 2,
      "stop": null,
      "stream": false,
      "system_message_ref": "physics_research_assistant",
      "provider": "openrouter",
      "sub_provider": "siliconflow/fp8",
      "quantization": "fp8",
      "sub_provider_model_endpoint": "deepseek-ai/DeepSeek-R1",
      "latency": "2.59s",
      "throughput": "22.54tps",
      "total_context": "163.8K",
      "max_output": "163.8K",
      "host_country": "",
      "input_price": "0.50",
      "output_price": "2.18"
    },
    "gemini-2.5-flash": {
      "model": "gemini-2.5-flash",
      "max_attempts": 3,
      "temperature": 0.5,
      "max_temperature": 2,
      "grounded": false,
      "provider": "gemini",
      "system_message_ref": "physics_research_assistant"
    },
    "gemini-2.5-flash-grounded": {
      "model": "gemini-2.5-flash",
      "max_attempts": 3,
      "temperature": 0.5,
      "max_temperature": 2,
      "grounded": true,
      "provider": "gemini",
      "system_message_ref": "physics_research_assistant"
    },
    "gemini-2.5-pro": {
      "model": "gemini-2.5-pro",
      "max_attempts": 3,
      "temperature": 0.75,
      "max_temperature": 2,
      "grounded": false,
      "provider": "gemini",
      "system_message_ref": "physics_research_assistant"
    },
    "gemini-2.5-pro-grounded": {
      "model": "gemini-2.5-pro",
      "max_attempts": 3,
      "temperature": 1.0,
      "max_temperature": 2,
      "grounded": true,
      "provider": "gemini",
      "system_message_ref": "physics_research_assistant"
    }
  }
}