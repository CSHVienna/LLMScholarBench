{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Alice and Alicia: 0.67\n",
      "Similarity between Alice and Bob: 0.00\n",
      "Similarity between Alice and Robert: 0.00\n",
      "Similarity between Alicia and Bob: 0.00\n",
      "Similarity between Alicia and Robert: 0.00\n",
      "Similarity between Bob and Robert: 0.33\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Levenshtein Distance (Edit Distance)\n",
    "Measures the minimum number of single-character edits required to transform one string into another. A higher distance indicates higher similarity.\n",
    "'''\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "names = [\"Alice\", \"Alicia\", \"Bob\", \"Robert\"]\n",
    "similarities = []\n",
    "\n",
    "# Compute pairwise similarity\n",
    "for i in range(len(names)):\n",
    "    for j in range(i + 1, len(names)):\n",
    "        sim = 1 - distance(names[i], names[j]) / max(len(names[i]), len(names[j]))\n",
    "        similarities.append(sim)\n",
    "        print(f\"Similarity between {names[i]} and {names[j]}: {sim:.2f}\")\n",
    "\n",
    "similarities = np.array(similarities)\n",
    "print(similarities.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Alice and Alicia: 0.46\n",
      "Similarity between Alice and Bob: 0.50\n",
      "Similarity between Alice and Robert: 0.42\n",
      "Similarity between Alicia and Bob: 0.20\n",
      "Similarity between Alicia and Robert: 0.41\n",
      "Similarity between Bob and Robert: 0.59\n",
      "0.57176983\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2. Cosine Similarity on Embeddings\n",
    "Convert names into numerical embeddings (e.g., using Word2Vec or Sentence Transformers) and compute cosine similarity.\n",
    "'''\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Pretrained embedding model\n",
    "names = [\"Alice\", \"Alicia\", \"Bob\", \"Robert\"]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(names)\n",
    "\n",
    "# Compute pairwise similarity\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "# Display similarity matrix\n",
    "for i, name1 in enumerate(names):\n",
    "    for j, name2 in enumerate(names):\n",
    "        if i < j:\n",
    "            print(f\"Similarity between {name1} and {name2}: {similarities[i][j]:.2f}\")\n",
    "print(similarities.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Alice and Alicia: 0.89\n",
      "Similarity between Alice and Bob: 0.00\n",
      "Similarity between Alice and Robert: 0.46\n",
      "Similarity between Alicia and Bob: 0.00\n",
      "Similarity between Alicia and Robert: 0.00\n",
      "Similarity between Bob and Robert: 0.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33592592592592596"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "3. Jaro-Winkler Similarity\n",
    "A metric that gives more weight to characters at the start of strings.\n",
    "'''\n",
    "import jellyfish\n",
    "import numpy as np\n",
    "\n",
    "names = [\"Alice\", \"Alicia\", \"Bob\", \"Robert\"]\n",
    "similarities = []\n",
    "\n",
    "# Compute pairwise similarity\n",
    "for i in range(len(names)):\n",
    "    for j in range(i + 1, len(names)):\n",
    "        sim = jellyfish.jaro_winkler_similarity(names[i], names[j])\n",
    "        similarities.append(sim)\n",
    "        print(f\"Similarity between {names[i]} and {names[j]}: {sim:.2f}\")\n",
    "\n",
    "# similarities = np.array(similarities)\n",
    "# print(similarities.mean())\n",
    "np.mean(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Alice and Alicia: 0.63\n",
      "Similarity between Alice and Bob: 0.00\n",
      "Similarity between Alice and Robert: 0.00\n",
      "Similarity between Alicia and Bob: 0.00\n",
      "Similarity between Alicia and Robert: 0.00\n",
      "Similarity between Bob and Robert: 0.19\n",
      "0.35279885975982334\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4. Token-Based Similarity (e.g., Jaccard Similarity)\n",
    "Splits names into tokens (characters or words) and measures overlap.\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "names = [\"Alice\", \"Alicia\", \"Bob\", \"Robert\"]\n",
    "\n",
    "# Vectorize names into character-level tokens\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))  # Bi-grams and tri-grams\n",
    "X = vectorizer.fit_transform(names)\n",
    "\n",
    "# Compute cosine similarity on tokenized names\n",
    "similarities = cosine_similarity(X)\n",
    "\n",
    "# Display similarity matrix\n",
    "for i, name1 in enumerate(names):\n",
    "    for j, name2 in enumerate(names):\n",
    "        if i < j:\n",
    "            print(f\"Similarity between {name1} and {name2}: {similarities[i][j]:.2f}\")\n",
    "\n",
    "print(similarities.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonetic Codes: {'Alice': 'A42', 'Alicia': 'A42', 'Bob': 'B1', 'Robert': 'R163'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "5. Phonetic Similarity (Soundex or Metaphone)\n",
    "Compares names based on how they sound rather than their spelling.\n",
    "'''\n",
    "\n",
    "from fuzzy import Soundex\n",
    "\n",
    "soundex = Soundex(4)\n",
    "names = [\"Alice\", \"Alicia\", \"Bob\", \"Robert\"]\n",
    "\n",
    "# Compute phonetic similarity\n",
    "phonetic_codes = {name: soundex(name) for name in names}\n",
    "print(\"Phonetic Codes:\", phonetic_codes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Diversity Index: 0.8037\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Shannon Diversity Index (Entropy)\n",
    "The Shannon Index measures uncertainty or entropy in the distribution of categories. \n",
    "It is sensitive to both richness (number of unique categories) and evenness (distribution of categories).\n",
    "'''\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "labels = ['cat', 'dog', 'dog', 'cat', 'bird', 'cat', 'bird', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog', 'dog']\n",
    "\n",
    "# Compute proportions\n",
    "counts = Counter(labels)\n",
    "total = sum(counts.values())\n",
    "proportions = [count / total for count in counts.values()]\n",
    "\n",
    "# Shannon Index\n",
    "shannon_index = -sum(p * np.log(p) for p in proportions)\n",
    "print(f\"Shannon Diversity Index: {shannon_index:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simpson's Diversity Index: 0.4567\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2. Simpson's Diversity Index\n",
    "Simpson's Index measures the probability that two individuals randomly selected from the dataset belong to the same category.\n",
    "'''\n",
    "\n",
    "# Simpson's Index\n",
    "simpson_index = 1 - sum(p**2 for p in proportions)\n",
    "print(f\"Simpson's Diversity Index: {simpson_index:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini-Simpson Index: 0.5433\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3. Gini-Simpson Index\n",
    "The Gini-Simpson Index is related to Simpson's Index and represents the probability that two randomly chosen individuals belong to different categories.\n",
    "'''\n",
    "# Gini-Simpson Index\n",
    "gini_simpson_index = sum(p**2 for p in proportions)\n",
    "print(f\"Gini-Simpson Index: {gini_simpson_index:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective Number of Species: 2.2339\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "4. Effective Number of Species\n",
    "The effective number of species converts the Shannon Index into the equivalent number of equally abundant categories.\n",
    "'''\n",
    "# Effective Number of Species\n",
    "effective_number_of_species = np.exp(shannon_index)\n",
    "print(f\"Effective Number of Species: {effective_number_of_species:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richness: 3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "5. Richness (Count of Unique Categories)\n",
    "Richness is the simplest diversity measure and represents the number of unique categories.\n",
    "'''\n",
    "# Richness\n",
    "richness = len(counts)\n",
    "print(f\"Richness: {richness}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scholarly similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity Score among selected scientists: 0.8404\n",
      "0.8563202927157882\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\", \"Frank\", \"Grace\", \"Hank\", \"Ivy\", \"Jack\"],\n",
    "    \"work_counts\": [10, 15, 20, 25, 30, 35, 40, 45, 50, 55],\n",
    "    \"cited_by_counts\": [200, 150, 300, 250, 400, 350, 500, 450, 600, 550],\n",
    "    \"h_index\": [5, 7, 6, 8, 10, 9, 11, 12, 13, 14],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Randomly select 10 scientists (or subset the DataFrame if already chosen)\n",
    "selected_scientists = df.sample(n=10, random_state=42)\n",
    "\n",
    "# Normalize the scholarly metrics\n",
    "scaler = MinMaxScaler()\n",
    "metrics = selected_scientists[[\"work_counts\", \"cited_by_counts\", \"h_index\"]]\n",
    "normalized_metrics = scaler.fit_transform(metrics)\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "similarity_matrix = cosine_similarity(normalized_metrics)\n",
    "\n",
    "# Compute average pairwise similarity\n",
    "pairwise_combinations = list(combinations(range(len(selected_scientists)), 2))\n",
    "average_similarity = np.mean([similarity_matrix[i, j] for i, j in pairwise_combinations])\n",
    "\n",
    "similarity = np.dot(normalized_metrics, normalized_metrics.T)\n",
    "\n",
    "print(f\"Average Similarity Score among selected scientists: {average_similarity:.4f}\")\n",
    "print(np.mean(similarity_matrix))\n",
    "\n",
    "# print(similarity)\n",
    "# print()\n",
    "# print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.]]), array([[1.]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[5,10,15,20]]), cosine_similarity([[1,5,10,15,20,60]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Euclidean Distance: 0.7213\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "distance_matrix = squareform(pdist(normalized_metrics, metric=\"euclidean\"))\n",
    "average_distance = np.mean([distance_matrix[i, j] for i, j in pairwise_combinations])\n",
    "print(f\"Average Euclidean Distance: {average_distance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.93939394, 0.97575758],\n",
       "       [0.93939394, 1.        , 0.91515152],\n",
       "       [0.97575758, 0.91515152, 1.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = np.corrcoef(normalized_metrics, rowvar=False)\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (5, 6), (5, 7), (5, 8), (5, 9), (6, 7), (6, 8), (6, 9), (7, 8), (7, 9), (8, 9)]\n",
      "Average Similarity Score in PCA Space: -0.1111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame (as before)\n",
    "data = {\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\", \"Frank\", \"Grace\", \"Hank\", \"Ivy\", \"Jack\"],\n",
    "    \"work_counts\": [10, 15, 20, 25, 30, 35, 40, 45, 50, 55],\n",
    "    \"cited_by_counts\": [200, 150, 300, 250, 400, 350, 500, 450, 600, 550],\n",
    "    \"h_index\": [5, 7, 6, 8, 10, 9, 11, 12, 13, 14],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Normalize metrics\n",
    "scaler = MinMaxScaler()\n",
    "metrics = df[[\"work_counts\", \"cited_by_counts\", \"h_index\"]]\n",
    "normalized_metrics = scaler.fit_transform(metrics)\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca_transformed = pca.fit_transform(normalized_metrics)\n",
    "\n",
    "# Add PCA components back to the DataFrame\n",
    "df[\"PC1\"], df[\"PC2\"] = pca_transformed[:, 0], pca_transformed[:, 1]\n",
    "\n",
    "# Randomly select 10 scientists\n",
    "selected_scientists = df.sample(n=10, random_state=42)\n",
    "\n",
    "# Compute similarity among the selected scientists in the 2D PCA space\n",
    "pca_metrics = selected_scientists[[\"PC1\", \"PC2\"]]\n",
    "similarity_matrix = cosine_similarity(pca_metrics)\n",
    "\n",
    "# Compute average pairwise similarity\n",
    "pairwise_combinations = list(combinations(range(len(selected_scientists)), 2))\n",
    "average_similarity = np.mean([similarity_matrix[i, j] for i, j in pairwise_combinations])\n",
    "\n",
    "print(pairwise_combinations)\n",
    "\n",
    "print(f\"Average Similarity Score in PCA Space: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        ],\n",
       "       [0.57945826, 0.58490361, 0.38018236],\n",
       "       [1.        , 1.        , 1.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example: Apply log transformation\n",
    "data = [[10, 1000, 5], [15, 1500, 6], [20, 2000, 8]]\n",
    "log_transformed = np.log1p(data)\n",
    "\n",
    "# Optional: Normalize after transformation\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(log_transformed)\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.47297643],\n",
       "       [-0.66976095],\n",
       "       [ 0.05534276],\n",
       "       [ 0.72739861],\n",
       "       [ 1.35999601]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "data = np.array([10, 15, 20, 25, 30])\n",
    "\n",
    "# Box-Cox transformation (requires all positive values)\n",
    "boxcox_transformed, _ = boxcox(data)\n",
    "\n",
    "# Normalize after transformation\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(boxcox_transformed.reshape(-1, 1))\n",
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.25039527, -1.25217013, -1.18159061],\n",
       "       [ 0.05302306,  0.056829  , -0.08217032],\n",
       "       [ 1.19737221,  1.19534113,  1.26376093]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Example: Yeo-Johnson transformation\n",
    "data = [[10, 1000, 5], [15, 1500, 6], [20, 2000, 8]]\n",
    "\n",
    "transformer = PowerTransformer(method='yeo-johnson')\n",
    "transformed_data = transformer.fit_transform(data)\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 3., 4., 5., 1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "# Example: Rank transformation\n",
    "data = [10, 15, 20, 100, 5]\n",
    "rank_transformed = rankdata(data, method='average')\n",
    "rank_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Index: 0.6619\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example series\n",
    "ages = pd.Series([25, 30, 35, 40, 45, 2])\n",
    "\n",
    "# Compute standard deviation\n",
    "std_dev = ages.std()\n",
    "\n",
    "# Normalized similarity index (1 means identical ages, closer to 0 means highly dispersed)\n",
    "similarity_index = 1 - (std_dev / ages.max())\n",
    "print(f\"Similarity Index: {similarity_index:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity on Age Distribution: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create age histogram (e.g., binning ages into 10-year intervals)\n",
    "age_bins = pd.cut(ages, bins=5, labels=False)  # 5 bins\n",
    "age_histogram = pd.Series(age_bins).value_counts().sort_index().values.reshape(1, -1)\n",
    "\n",
    "# Compute cosine similarity (self-similarity for a single series = 1)\n",
    "cosine_sim = cosine_similarity(age_histogram, age_histogram)[0, 0]\n",
    "print(f\"Cosine Similarity on Age Distribution: {cosine_sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini-Based Similarity Index: 0.7505\n"
     ]
    }
   ],
   "source": [
    "def gini_coefficient(array):\n",
    "    sorted_array = np.sort(array)\n",
    "    n = len(array)\n",
    "    cumulative_sum = np.cumsum(sorted_array)\n",
    "    gini = (2 / n) * np.sum((np.arange(1, n + 1) * sorted_array)) / cumulative_sum[-1] - (n + 1) / n\n",
    "    return gini\n",
    "\n",
    "# Compute Gini coefficient\n",
    "gini = gini_coefficient(ages.values)\n",
    "similarity_index = 1 - gini  # Transform to similarity (1 = identical ages, 0 = high disparity)\n",
    "print(f\"Gini-Based Similarity Index: {similarity_index:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cosine Similarity (Z-scores): 0.7656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "ages = pd.Series([1,1,1,1,1,1,1,0])\n",
    "\n",
    "# Z-score normalization\n",
    "# z_scores = (ages - ages.mean()) / ages.std()\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "z_scores_matrix = ages.values.reshape(-1, 1)\n",
    "cosine_sim = cosine_similarity(z_scores_matrix)\n",
    "average_similarity = cosine_sim.mean()\n",
    "\n",
    "print(f\"Average Cosine Similarity (Z-scores): {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   NaN\n",
       "1   NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Authors:\n",
      "Author 1: ['Univ A', 'Univ B']\n",
      "Author 5: ['Univ F']\n",
      "Author 2: ['Univ A', 'Univ C']\n",
      "\n",
      "Pairwise Jaccard Similarities: [0.0, 0.3333333333333333, 0.0]\n",
      "Average Jaccard Similarity: 0.1111\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from itertools import combinations\n",
    "\n",
    "# Example data: List of authors and their affiliations\n",
    "authors = [\n",
    "    {\"name\": \"Author 1\", \"affiliations\": [\"Univ A\", \"Univ B\"]},\n",
    "    {\"name\": \"Author 2\", \"affiliations\": [\"Univ A\", \"Univ C\"]},\n",
    "    {\"name\": \"Author 3\", \"affiliations\": [\"Univ D\", \"Univ E\"]},\n",
    "    {\"name\": \"Author 4\", \"affiliations\": [\"Univ A\", \"Univ B\", \"Univ C\"]},\n",
    "    {\"name\": \"Author 5\", \"affiliations\": [\"Univ F\"]},\n",
    "]\n",
    "\n",
    "# 1. Sample a subset of authors\n",
    "sample_size = 3\n",
    "sampled_authors = random.sample(authors, sample_size)\n",
    "\n",
    "# 2. Define a Jaccard similarity function for two lists\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# 3. Compute pairwise Jaccard similarity\n",
    "pairwise_similarities = []\n",
    "for author1, author2 in combinations(sampled_authors, 2):\n",
    "    similarity = jaccard_similarity(author1[\"affiliations\"], author2[\"affiliations\"])\n",
    "    pairwise_similarities.append(similarity)\n",
    "\n",
    "# 4. Calculate the average Jaccard similarity\n",
    "average_similarity = sum(pairwise_similarities) / len(pairwise_similarities) if pairwise_similarities else 0\n",
    "\n",
    "# Output the results\n",
    "print(\"Sampled Authors:\")\n",
    "for author in sampled_authors:\n",
    "    print(f\"{author['name']}: {author['affiliations']}\")\n",
    "\n",
    "print(f\"\\nPairwise Jaccard Similarities: {pairwise_similarities}\")\n",
    "print(f\"Average Jaccard Similarity: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import permutations\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    " \n",
    "import sys\n",
    "sys.path.append('../../code/')\n",
    "\n",
    "from libs import io\n",
    "from libs import constants\n",
    "from postprocessing import similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_os_data_tar_gz = '../../data/final_dataset.tar.gz'\n",
    "llm_valid_csv = '../../results/factuality/llama-3.1-8b_author.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authorships = io.read_file_from_tar_gz_as_dataframe(aps_os_data_tar_gz, constants.APS_OA_AUTHORSHIPS_FN)\n",
    "df_authorships.rename(columns={'id_author':'id_author_oa', 'id_institution':'id_institution_oa'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_institutions = io.read_file_from_tar_gz_as_dataframe(aps_os_data_tar_gz, constants.APS_OA_INSTITUTIONS_FN)\n",
    "df_institutions.rename(columns={'id_institution':'id_institution_oa'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>llm_model</th>\n",
       "      <th>task_name</th>\n",
       "      <th>task_param</th>\n",
       "      <th>task_attempt</th>\n",
       "      <th>result_valid_flag</th>\n",
       "      <th>name</th>\n",
       "      <th>years</th>\n",
       "      <th>doi</th>\n",
       "      <th>...</th>\n",
       "      <th>h_index</th>\n",
       "      <th>i10_index</th>\n",
       "      <th>e_index</th>\n",
       "      <th>two_year_mean_citedness</th>\n",
       "      <th>year_first_publication</th>\n",
       "      <th>year_last_publication</th>\n",
       "      <th>academic_age</th>\n",
       "      <th>age_now</th>\n",
       "      <th>seniority_active</th>\n",
       "      <th>seniority_now</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-12-09</td>\n",
       "      <td>00:00</td>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>top_k</td>\n",
       "      <td>top_5</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "      <td>Stephen Hawking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>106.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>261.326342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-12-09</td>\n",
       "      <td>00:00</td>\n",
       "      <td>llama-3.1-8b-instant</td>\n",
       "      <td>top_k</td>\n",
       "      <td>top_5</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "      <td>Richard Feynman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>507.631749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1939.0</td>\n",
       "      <td>1986.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date   time             llm_model task_name task_param  \\\n",
       "10  2024-12-09  00:00  llama-3.1-8b-instant     top_k      top_5   \n",
       "11  2024-12-09  00:00  llama-3.1-8b-instant     top_k      top_5   \n",
       "\n",
       "    task_attempt result_valid_flag             name years  doi  ... h_index  \\\n",
       "10             1             valid  Stephen Hawking   NaN  NaN  ...   106.0   \n",
       "11             1             valid  Richard Feynman   NaN  NaN  ...    88.0   \n",
       "\n",
       "   i10_index     e_index  two_year_mean_citedness  year_first_publication  \\\n",
       "10     221.0  261.326342                      0.0                  1965.0   \n",
       "11     148.0  507.631749                      0.0                  1939.0   \n",
       "\n",
       "    year_last_publication academic_age age_now seniority_active seniority_now  \n",
       "10                 2016.0         52.0    61.0           senior        senior  \n",
       "11                 1986.0         48.0    87.0           senior        senior  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses = io.read_csv(llm_valid_csv, index_col=None).drop(columns=['Unnamed: 0'])\n",
    "df_responses = df_responses.query(\"task_name == 'top_k'\")\n",
    "df_responses.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'time', 'llm_model', 'task_name', 'task_param', 'task_attempt',\n",
       "       'result_valid_flag', 'name', 'years', 'doi', 'career_age', 'clean_name',\n",
       "       'model', 'valid_attempt', 'id_author_oa', 'fact_author_score',\n",
       "       'id_author_aps_list', 'ethnicity_dx', 'ethnicity_ec', 'ethnicity',\n",
       "       'gender', 'works_count', 'cited_by_count', 'h_index', 'i10_index',\n",
       "       'e_index', 'two_year_mean_citedness', 'year_first_publication',\n",
       "       'year_last_publication', 'academic_age', 'age_now', 'seniority_active',\n",
       "       'seniority_now'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_responses.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_group(group, df_authorships, df_institutions):\n",
    "\n",
    "#     print('==========================')\n",
    "\n",
    "#     clean_group = group.dropna(subset=['id_author_oa'])\n",
    "#     n_unique_author_recommendations = clean_group.id_author_oa.astype(int).nunique()\n",
    "   \n",
    "#     if clean_group.empty or n_unique_author_recommendations == 1:\n",
    "#         institutions_share = None\n",
    "#         coauthors_share = None\n",
    "#         country_of_affiliation_share = None\n",
    "    \n",
    "#     else:\n",
    "        \n",
    "#         # insitutions and coauthors in common\n",
    "\n",
    "#         print('group:\\n', group[['name','id_author_oa']])\n",
    "#         print('\\n id_author_oa:', group.id_author_oa.nunique())\n",
    "\n",
    "#         ids = clean_group.id_author_oa.dropna().unique()\n",
    "#         df_authorships_filtered = df_authorships.query('id_author_oa in @ids').dropna(subset=['id_institution_oa'])\n",
    "\n",
    "#         # df_institutions_authors = df_authorships_filtered[['id_author_oa','id_institution_oa']].drop_duplicates().groupby('id_author_oa').id_institution_oa.apply(list).reset_index(name='_items').astype(str).set_index('id_author_oa')\n",
    "#         # df_institutions_authors._items = df_institutions_authors._items.astype(int)\n",
    "#         df_institutions_authors = similarity.get_items_by_author(df_authorships_filtered.groupby('id_author_oa').id_institution_oa.unique(), df_institutions, 'id_institution_oa')\n",
    "#         institutions_share = similarity.compute_average_jaccard_similarity(df_institutions_authors)\n",
    "        \n",
    "#         # print(\"\\n df_institutions_authors:\\n\", df_institutions_authors)\n",
    "#         # print('institutions_share:\\n', institutions_share)\n",
    "\n",
    "\n",
    "#         df_countries = similarity.get_items_by_author(df_authorships_filtered.groupby('id_author_oa').id_institution_oa.unique(), df_institutions, 'country_code')\n",
    "#         country_of_affiliation_share = similarity.compute_average_jaccard_similarity(df_countries)\n",
    "\n",
    "#         # print(\"\\n df_countries:\\n\", df_countries)\n",
    "#         # print('country_of_affiliation_share:\\n', country_of_affiliation_share)\n",
    "\n",
    "\n",
    "#         df_coauthors = similarity.get_items_by_author(df_authorships_filtered.groupby('id_author_oa').id_institution_oa.unique(), df_authorships, 'id_author_oa', column_item_cast=int)\n",
    "#         coauthors_share = similarity.compute_average_jaccard_similarity(df_coauthors)\n",
    "\n",
    "#         print(\"\\n df_coauthors:\\n\", df_coauthors)\n",
    "#         print('coauthors_share:\\n', coauthors_share)\n",
    "        \n",
    "#         # coauthors among the recommendations\n",
    "#         df_coauthors_recommended = pd.DataFrame(df_coauthors.apply(lambda row: list(set(row._items).intersection(set(ids)) - set([row.name])), axis=1), columns=['_items'])\n",
    "#         coauthors_share_recommended = similarity.compute_average_jaccard_similarity(df_coauthors_recommended)\n",
    "\n",
    "#         print(ids)\n",
    "#         print(\"\\n df_coauthors_recommended:\\n\", df_coauthors_recommended)\n",
    "#         print('coauthors_share_recommended:\\n', coauthors_share_recommended)\n",
    "\n",
    "#         # import sys\n",
    "#         # sys.exit(0)\n",
    "        \n",
    "#     # Return a DataFrame with one row and multiple columns\n",
    "#     df = pd.DataFrame({\n",
    "#         'institutions_share': [institutions_share],\n",
    "#         'country_of_affiliation_share': [country_of_affiliation_share],\n",
    "#         'coauthors_share': [coauthors_share]\n",
    "#     })\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(group, df_authorships, df_institutions):    \n",
    "    # Remove rows with missing author ids\n",
    "    clean_group = group.dropna(subset=['id_author_oa'])\n",
    "    \n",
    "    # Compute the number of name recommendations and author hallucinations\n",
    "    n_unique_author_recommendations = clean_group.id_author_oa.nunique()\n",
    "    \n",
    "    if clean_group.empty or n_unique_author_recommendations == 1:\n",
    "        gender_diversity = None\n",
    "        ethnicity_diversity = None\n",
    "        scholarly_similarity = None\n",
    "        aps_similarity = None\n",
    "        oa_similarity = None\n",
    "        aps_career_age_similarity = None\n",
    "        oa_career_age_similarity = None\n",
    "        institutions_share = None\n",
    "        coauthors_share = None\n",
    "        country_of_affiliation_share = None\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        \n",
    "        ids = clean_group.id_author_oa.dropna().unique()\n",
    "        df_authorships_filtered = df_authorships.query('id_author_oa in @ids').dropna(subset=['id_institution_oa'])\n",
    "\n",
    "        # shared institutions\n",
    "        df_institutions_authors = similarity.get_items_by_author(df_authorships_filtered.groupby('id_author_oa').id_institution_oa.unique(), df_institutions, 'id_institution_oa')\n",
    "        institutions_share = similarity.compute_average_jaccard_similarity(df_institutions_authors)\n",
    "        \n",
    "        #print('institutions_share:', institutions_share)\n",
    "\n",
    "        # shared institutions' countries\n",
    "        df_countries = similarity.get_items_by_author(df_authorships_filtered.groupby('id_author_oa').id_institution_oa.unique(), df_institutions, 'country_code')\n",
    "        country_of_affiliation_share = similarity.compute_average_jaccard_similarity(df_countries)\n",
    "\n",
    "        # print('country_of_affiliation_share:', country_of_affiliation_share)\n",
    "\n",
    "        # shared coauthors\n",
    "        df_coauthors = similarity.get_items_by_author(df_authorships_filtered.groupby('id_author_oa').id_institution_oa.unique(), df_authorships, 'id_author_oa', column_item_cast=int)\n",
    "        coauthors_share = similarity.compute_average_jaccard_similarity(df_coauthors)\n",
    "\n",
    "        # print('coauthors_share:', coauthors_share)\n",
    "\n",
    "        # coauthors among the recommendations\n",
    "        df_coauthors_recommended = pd.DataFrame(df_coauthors.apply(lambda row: list(set(row._items).intersection(set(ids)) - set([row.name])), axis=1), columns=['_items'])\n",
    "        coauthors_recommended_share = similarity.compute_average_jaccard_similarity(df_coauthors_recommended)\n",
    "        \n",
    "        all_possible_pairs = len(list(permutations(ids, 2)))\n",
    "        recommended_authors_are_coauthors = df_coauthors_recommended._items.apply(lambda x: len(x) > 0).sum() / all_possible_pairs\n",
    "\n",
    "        if coauthors_recommended_share > 0:\n",
    "            print(df_coauthors_recommended)\n",
    "\n",
    "            print('recommended_authors_are_coauthors:', recommended_authors_are_coauthors)\n",
    "            print('coauthors_recommended_share: ', coauthors_recommended_share)\n",
    "\n",
    "            import sys\n",
    "            sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/81 [00:03<05:08,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                         _items\n",
      "id_author_oa                                                   \n",
      "5001536933    [5004950660.0, 5110862535.0, 5106849896.0, 503...\n",
      "5004950660    [5043077347.0, 5012326853.0, 5051731365.0, 500...\n",
      "5006656464    [5004950660.0, 5012326853.0, 5110862535.0, 511...\n",
      "5012326853    [5004950660.0, 5110862535.0, 5041516104.0, 511...\n",
      "5012461168                                                   []\n",
      "5014383473    [5012326853.0, 5110862535.0, 5041516104.0, 511...\n",
      "5017158718                                                   []\n",
      "5023914308                                                   []\n",
      "5027643641                                       [5043077347.0]\n",
      "5029910389                                                   []\n",
      "5031678863                                                   []\n",
      "5032512426    [5110436352.0, 5012326853.0, 5110862535.0, 511...\n",
      "5037710835    [5004950660.0, 5012326853.0, 5110862535.0, 504...\n",
      "5041516104    [5012326853.0, 5066175077.0, 5045842412.0, 501...\n",
      "5043077347    [5113446179.0, 5004950660.0, 5012326853.0, 505...\n",
      "5044004887                                                   []\n",
      "5045721355    [5006656464.0, 5111474378.0, 5037710835.0, 511...\n",
      "5045842412    [5012326853.0, 5066175077.0, 5110862535.0, 504...\n",
      "5046315734    [5057252770.0, 5012326853.0, 5066175077.0, 511...\n",
      "5048800703                         [5105523538.0, 5053459131.0]\n",
      "5051731365    [5110436352.0, 5004950660.0, 5012326853.0, 511...\n",
      "5053459131    [5057252770.0, 5012326853.0, 5066175077.0, 511...\n",
      "5057252770    [5113446179.0, 5091196459.0, 5006656464.0, 504...\n",
      "5057977946                                                   []\n",
      "5065234398    [5113446179.0, 5091196459.0, 5110170702.0, 501...\n",
      "5066175077    [5078382819.0, 5012326853.0, 5041516104.0, 504...\n",
      "5077199483                                                   []\n",
      "5078382819    [5110436352.0, 5051731365.0, 5066175077.0, 511...\n",
      "5081108187                                                   []\n",
      "5091196459    [5110436352.0, 5004950660.0, 5012326853.0, 511...\n",
      "5101843634                                                   []\n",
      "5103436661    [5004950660.0, 5012326853.0, 5110862535.0, 511...\n",
      "5103437173                                                   []\n",
      "5103644440                                       [5065234398.0]\n",
      "5103986577                         [5105465619.0, 5045842412.0]\n",
      "5105465619                         [5103986577.0, 5045842412.0]\n",
      "5105523538                         [5053459131.0, 5048800703.0]\n",
      "5106849896    [5001536933.0, 5032512426.0, 5091196459.0, 511...\n",
      "5109058842                                                   []\n",
      "5109805546    [5110436352.0, 5078382819.0, 5051731365.0, 511...\n",
      "5109971261    [5110436352.0, 5078382819.0, 5051731365.0, 501...\n",
      "5110090983    [5110436352.0, 5078382819.0, 5051731365.0, 501...\n",
      "5110170702    [5043077347.0, 5032512426.0, 5103436661.0, 510...\n",
      "5110436352    [5078382819.0, 5051731365.0, 5110090983.0, 503...\n",
      "5110862535    [5004950660.0, 5012326853.0, 5111474378.0, 504...\n",
      "5111474378    [5004950660.0, 5012326853.0, 5110862535.0, 504...\n",
      "5112582417    [5001536933.0, 5110862535.0, 5106849896.0, 503...\n",
      "5113446179    [5057252770.0, 5043077347.0, 5051731365.0, 509...\n",
      "5113930228    [5001536933.0, 5110862535.0, 5106849896.0, 503...\n",
      "recommended_authors_are_coauthors: 0.013951734539969835\n",
      "coauthors_recommended_share:  0.11109604774060641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/espinl/anaconda3/envs/py311LLMEval/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "cols = ['model', 'task_name', 'task_param', 'date', 'time']\n",
    "df_request_stats = df_responses.groupby(cols).progress_apply(lambda row: process_group(row, \n",
    "                                                                                        df_authorships=df_authorships,\n",
    "                                                                                        df_institutions=df_institutions,\n",
    "                                                                                        )).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311LLMEval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
